{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Yelp Sentiment Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the data set\n",
    "Here we explore the yelp dataset review. We use a dataset with Yelp reviews and a 1-5 star rating associated with it to learn how to use the bag of words model to conduct sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment to use Yelp Reviews dataset\n",
    "# df = pd.read_csv('yelp_academic_dataset_review.csv') \n",
    "###\n",
    "\n",
    "### Uncomment this to use the Airplane Tweets dataset\n",
    "df = pd.read_csv('Tweets.csv') \n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply grab all the one star and five star data from the dataset here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame of reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11423</th>\n",
       "      <td>-1</td>\n",
       "      <td>@USAirways a $100 @Samsonite - totaled. Not ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13914</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir ok. Pilots are looking for agents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3787</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united rebooked.  This one is Late Flight too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united working with Lisa J at ORD. she's work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united can you get a gate for UA4727?  Turrible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8438</th>\n",
       "      <td>1</td>\n",
       "      <td>I know you have a lot of baggage... But i want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7551</th>\n",
       "      <td>-1</td>\n",
       "      <td>@JetBlue they are now being sent off the plane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6768</th>\n",
       "      <td>-1</td>\n",
       "      <td>@JetBlue upset with the lack of communication ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6189</th>\n",
       "      <td>-1</td>\n",
       "      <td>@SouthwestAir I'm smashed into a window by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united I did and then she made me feel guilty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united, no, your service here pretty much rui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5744</th>\n",
       "      <td>-1</td>\n",
       "      <td>@SouthwestAir hello. my flight is Cancelled Fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6581</th>\n",
       "      <td>-1</td>\n",
       "      <td>@SouthwestAir missing out on over 6,000 points...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10252</th>\n",
       "      <td>-1</td>\n",
       "      <td>@USAirways @AmericanAir Suggestions , been on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3830</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united a plane took our gate and now we're ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united 6377 and now they sent our green tagge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united I have received one previously on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11968</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir I am looking for help on USAirway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica thank you for the easy itinerar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>1</td>\n",
       "      <td>@united thank you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united no because you will charge me or delay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united I fly @AmericanAir normally.  This doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14266</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir been a very long day in Philly......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7779</th>\n",
       "      <td>-1</td>\n",
       "      <td>@JetBlue at what point do u Cancelled Flight f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14179</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir I've been on hold for about 3 and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united Waiting half an hour for my checked ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9165</th>\n",
       "      <td>-1</td>\n",
       "      <td>@USAirways  I'm glad this airline is going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12189</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir \\nNot giving you a hard time...Ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>-1</td>\n",
       "      <td>@SouthwestAir every other city on your nationw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12033</th>\n",
       "      <td>1</td>\n",
       "      <td>@AmericanAir Thank you.....you do the same!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>1</td>\n",
       "      <td>@united I was sincerely thanking the pilot of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united I hope your corporate office is ready ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12065</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir thx for responding. I cant watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica I love the dancing little richa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8574</th>\n",
       "      <td>1</td>\n",
       "      <td>@JetBlue okay. The new screens are laptop-larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12892</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir been calling all morning. Still c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13193</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir literally just stopped allowing p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13863</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir i hope it takes you 6 hours to ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united sorry for the delayed response. It was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9844</th>\n",
       "      <td>-1</td>\n",
       "      <td>@USAirways really? 8 hr delay in Virgin Island...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united Friend at O'Hare and can't get on flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united. Is it reasonable to wait 45 mins for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united what a nightmare!!  Both sides of my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>-1</td>\n",
       "      <td>@SouthwestAir, on #unscheduled aircraft change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10811</th>\n",
       "      <td>-1</td>\n",
       "      <td>@USAirways #2066. Was on plane from PBI to CLT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6347</th>\n",
       "      <td>1</td>\n",
       "      <td>@SouthwestAir thanks for getting me back to Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14092</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir a worker just checked for ice on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you for your help, Shannon! Great custom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405</th>\n",
       "      <td>-1</td>\n",
       "      <td>@JetBlue I have been on phone with rep for ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4531</th>\n",
       "      <td>-1</td>\n",
       "      <td>@SouthwestAir has become like every other airl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14005</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir usually raving about the service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7834</th>\n",
       "      <td>1</td>\n",
       "      <td>@JetBlue thanks so much for your condolences a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>-1</td>\n",
       "      <td>@SouthwestAir customer service you shouldn't u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united loses my luggage and @hotelstonight lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8554</th>\n",
       "      <td>1</td>\n",
       "      <td>@JetBlue Done! Also looks like you opened some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14128</th>\n",
       "      <td>-1</td>\n",
       "      <td>@AmericanAir i do not want to stay overnight a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9284</th>\n",
       "      <td>-1</td>\n",
       "      <td>@USAirways @Gregm528 well there is certainly n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9817</th>\n",
       "      <td>-1</td>\n",
       "      <td>@USAirways your IVR hung up on me because of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788</th>\n",
       "      <td>-1</td>\n",
       "      <td>@jetblue having trouble signing in to TrueBlue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4320</th>\n",
       "      <td>-1</td>\n",
       "      <td>@united I tried 2 DM it would not go thru... n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11541 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                               text\n",
       "11423     -1  @USAirways a $100 @Samsonite - totaled. Not ha...\n",
       "13914     -1  @AmericanAir ok. Pilots are looking for agents...\n",
       "3787      -1    @united rebooked.  This one is Late Flight too.\n",
       "1238      -1  @united working with Lisa J at ORD. she's work...\n",
       "3633      -1  @united can you get a gate for UA4727?  Turrible.\n",
       "8438       1  I know you have a lot of baggage... But i want...\n",
       "7551      -1  @JetBlue they are now being sent off the plane...\n",
       "6768      -1  @JetBlue upset with the lack of communication ...\n",
       "6189      -1  @SouthwestAir I'm smashed into a window by the...\n",
       "3017      -1  @united I did and then she made me feel guilty...\n",
       "2333      -1  @united, no, your service here pretty much rui...\n",
       "5744      -1  @SouthwestAir hello. my flight is Cancelled Fl...\n",
       "6581      -1  @SouthwestAir missing out on over 6,000 points...\n",
       "10252     -1  @USAirways @AmericanAir Suggestions , been on ...\n",
       "3830      -1  @united a plane took our gate and now we're ju...\n",
       "2139      -1  @united 6377 and now they sent our green tagge...\n",
       "3931      -1  @united I have received one previously on the ...\n",
       "11968     -1  @AmericanAir I am looking for help on USAirway...\n",
       "252        1  @VirginAmerica thank you for the easy itinerar...\n",
       "867        1                                 @united thank you.\n",
       "3014      -1  @united no because you will charge me or delay...\n",
       "2354      -1  @united I fly @AmericanAir normally.  This doe...\n",
       "14266     -1  @AmericanAir been a very long day in Philly......\n",
       "7779      -1  @JetBlue at what point do u Cancelled Flight f...\n",
       "14179     -1  @AmericanAir I've been on hold for about 3 and...\n",
       "1787      -1  @united Waiting half an hour for my checked ba...\n",
       "9165      -1  @USAirways  I'm glad this airline is going to ...\n",
       "12189     -1  @AmericanAir \\nNot giving you a hard time...Ju...\n",
       "5466      -1  @SouthwestAir every other city on your nationw...\n",
       "12033      1       @AmericanAir Thank you.....you do the same!!\n",
       "...      ...                                                ...\n",
       "1486       1  @united I was sincerely thanking the pilot of ...\n",
       "2122      -1  @united I hope your corporate office is ready ...\n",
       "12065     -1  @AmericanAir thx for responding. I cant watch ...\n",
       "465        1  @VirginAmerica I love the dancing little richa...\n",
       "8574       1  @JetBlue okay. The new screens are laptop-larg...\n",
       "12892     -1  @AmericanAir been calling all morning. Still c...\n",
       "13193     -1  @AmericanAir literally just stopped allowing p...\n",
       "13863     -1  @AmericanAir i hope it takes you 6 hours to ge...\n",
       "1148      -1  @united sorry for the delayed response. It was...\n",
       "9844      -1  @USAirways really? 8 hr delay in Virgin Island...\n",
       "2424      -1  @united Friend at O'Hare and can't get on flig...\n",
       "1202      -1  @united. Is it reasonable to wait 45 mins for ...\n",
       "2194      -1  @united what a nightmare!!  Both sides of my f...\n",
       "5822      -1  @SouthwestAir, on #unscheduled aircraft change...\n",
       "10811     -1  @USAirways #2066. Was on plane from PBI to CLT...\n",
       "6347       1  @SouthwestAir thanks for getting me back to Na...\n",
       "14092     -1  @AmericanAir a worker just checked for ice on ...\n",
       "5627       1  Thank you for your help, Shannon! Great custom...\n",
       "8405      -1  @JetBlue I have been on phone with rep for ove...\n",
       "4531      -1  @SouthwestAir has become like every other airl...\n",
       "14005     -1  @AmericanAir usually raving about the service ...\n",
       "7834       1  @JetBlue thanks so much for your condolences a...\n",
       "4497      -1  @SouthwestAir customer service you shouldn't u...\n",
       "3519      -1  @united loses my luggage and @hotelstonight lo...\n",
       "8554       1  @JetBlue Done! Also looks like you opened some...\n",
       "14128     -1  @AmericanAir i do not want to stay overnight a...\n",
       "9284      -1  @USAirways @Gregm528 well there is certainly n...\n",
       "9817      -1  @USAirways your IVR hung up on me because of h...\n",
       "6788      -1  @jetblue having trouble signing in to TrueBlue...\n",
       "4320      -1  @united I tried 2 DM it would not go thru... n...\n",
       "\n",
       "[11541 rows x 2 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### UNCOMMENT FOR YELP DATASET\n",
    "# # Get one star reviews and label them with -1\n",
    "# dfOnes = df[df['stars'] == 1]\n",
    "# dfOnes = dfOnes.head(10000)\n",
    "# dfOnes['stars'] = dfOnes['stars'].apply(lambda x: -1)\n",
    "\n",
    "# # Get five star reviews and label them with 1\n",
    "# print(\"Shape of the ones input: \")\n",
    "# print(dfOnes.shape)\n",
    "# dfFives = df[df['stars'] == 5]\n",
    "# dfFives = dfFives.head(10000)\n",
    "# dfFives['stars'] = dfFives['stars'].apply(lambda x: 1)\n",
    "\n",
    "# print(\"Shape of the fives input: \")\n",
    "# print(dfFives.shape)\n",
    "# dfCombined = pd.concat([dfOnes, dfFives], axis=0)\n",
    "\n",
    "# dfCombined=dfCombined.rename(columns = {'stars':'class'})\n",
    "###\n",
    "\n",
    "\n",
    "### UNCOMMENT FOR AIRLINE DATA\n",
    "dfCombined = df[['airline_sentiment', 'text']]\n",
    "dfCombined = dfCombined[dfCombined.airline_sentiment != 'neutral']\n",
    "dfCombined['airline_sentiment'] = dfCombined['airline_sentiment'].replace(['positive','negative'],[1, -1])\n",
    "dfCombined=dfCombined.rename(columns = {'airline_sentiment':'class'})\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfCombined = dfCombined.sample(frac=1)\n",
    "\n",
    "dfTrainset = dfCombined.head(int(len(dfCombined.index) * .8))\n",
    "dfTestset = dfCombined.tail(int(len(dfCombined.index) * .2))\n",
    "\n",
    "trainX = np.asarray(dfTrainset['text'])\n",
    "trainY = np.asarray(dfTrainset['class'])\n",
    "\n",
    "testX = np.asarray(dfTestset['text'])\n",
    "testY = np.asarray(dfTestset['class'])\n",
    "\n",
    "print('Data Frame of reviews:')\n",
    "\n",
    "\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Review Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run the below block multiple times to see different reviews and the classification assigned to the review. Please comment below on what interesting aspects of the reviews you find to lead to certain classifications. What distinguishes between a classification of 1 and one of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: @JetBlue 2 words: \"staff training.\" What a joke-\n",
      "\n",
      "Classification: -1\n"
     ]
    }
   ],
   "source": [
    "sample = dfCombined.sample() \n",
    "print(\"Text: \" + sample['text'].values[0]  + \"\\n\")\n",
    "print(\"Classification: \" + str(sample['class'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any meaningful answer that discusses words that are more commonly used in positive reviews vs\n",
    "# negative reviews or length analysis will suffice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Corpus Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking of the combined string of every review in the usable dataset. In the provided space below use a histogram to visualize the frequency of the 25 most common words. Then answer the questions that follow. Hint: The most_common funtion for the counter may come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allText = ' '.join(dfCombined[\"text\"])\n",
    "words = allText.split() \n",
    "\n",
    "wordCounts = Counter()\n",
    "for word in words:\n",
    "    wordCounts[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all text:\n",
      "1260768\n",
      "Number of unique words:\n",
      "25314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 25 artists>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABrgAAAI/CAYAAAAsrNnNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdX4xmd33f8c+3TKAVrfA6TC3XNjJSVonIBeCOjKNUVYsV/6FV7IsUuarKClnaXqAqkSq10JvRQiLBTWi4iCUrpl2iJI5Li2xFKO7KIPWmgMeBOgGCvCGx7JX/bFnjtLVKC/32Yo6TAXa7M3h2n/3OvF7S6Dnnd37Ps79z4wu/9TunujsAAAAAAAAwxV9Z9QIAAAAAAABgLwQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEZZW/UC/n/e/OY394033rjqZQAAAAAAAHCZPfHEE/+tu9fPd+2KDlw33nhjtra2Vr0MAAAAAAAALrOqevpC1zyiEAAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYJSLBq6q+smq+sqOvz+vql+qqqur6lRVPbV8HlnmV1V9oqpOV9WTVXXTjt86tsx/qqqOXcobAwAAAAAA4GC6aODq7m909zu6+x1J/naSV5J8JskHkzzW3UeTPLacJ8mdSY4uf8eT3JckVXV1ks0k70pyc5LNV6MYAAAAAAAA7NZeH1F4a5I/6e6nk9yV5OQyfjLJ3cvxXUk+1du+kOSqqro2ye1JTnX3ue5+KcmpJHe85jsAAAAAAADgUNlr4Lonye8sx9d093PL8fNJrlmOr0vyzI7vPLuMXWgcAAAAAAAAdm3XgauqXp/k55P8+x+81t2dpPdjQVV1vKq2qmrr7Nmz+/GTAAAAAAAAHCB72cF1Z5I/6O4XlvMXlkcPZvl8cRk/k+SGHd+7fhm70Pj36e77u3ujuzfW19f3sDwAAAAAAAAOg70Ern+cv3w8YZI8kuTYcnwsycM7xt9X225J8vLyKMNHk9xWVUeq6kiS25YxAAAAAAAA2LW13Uyqqjcm+bkk/2zH8EeTPFRV9yZ5Osl7l/HPJnlPktNJXkny/iTp7nNV9ZEkjy/zPtzd517zHQAAAAAAAHCo1Pbrs65MGxsbvbW1teplAAAAAAAAcJlV1RPdvXG+a3t5RCEAAAAAAACsnMAFAAAAAADAKAIXAAAAAAAAo6ytegHsnzpRq17CZdebV+475AAAAAAAgEvDDi4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYZVeBq6quqqpPV9UfV9XXq+pnqurqqjpVVU8tn0eWuVVVn6iq01X1ZFXdtON3ji3zn6qqY5fqpgAAAAAAADi4druD69eS/H53/1SStyf5epIPJnmsu48meWw5T5I7kxxd/o4nuS9JqurqJJtJ3pXk5iSbr0YxAAAAAAAA2K2LBq6qelOSv5vkgSTp7v/d3d9OcleSk8u0k0nuXo7vSvKp3vaFJFdV1bVJbk9yqrvPdfdLSU4luWNf7wYAAAAAAIADbzc7uN6a5GySf1tVX66q36iqNya5prufW+Y8n+Sa5fi6JM/s+P6zy9iFxgEAAAAAAGDXdhO41pLclOS+7n5nkv+Zv3wcYZKkuztJ78eCqup4VW1V1dbZs2f34ycBAAAAAAA4QHYTuJ5N8mx3f3E5/3S2g9cLy6MHs3y+uFw/k+SGHd+/fhm70Pj36e77u3ujuzfW19f3ci8AAAAAAAAcAhcNXN39fJJnquonl6Fbk3wtySNJji1jx5I8vBw/kuR9te2WJC8vjzJ8NMltVXWkqo4kuW0ZAwAAAAAAgF1b2+W8f57kt6rq9Um+meT92Y5jD1XVvUmeTvLeZe5nk7wnyekkryxz093nquojSR5f5n24u8/ty10AAAAAAABwaOwqcHX3V5JsnOfSreeZ20k+cIHf+WSST+5lgQAAAAAAALDTbt7BBQAAAAAAAFcMgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUXYVuKrqz6rqD6vqK1W1tYxdXVWnquqp5fPIMl5V9YmqOl1VT1bVTTt+59gy/6mqOnZpbgkAAAAAAICDbC87uP5+d7+juzeW8w8meay7jyZ5bDlPkjuTHF3+jie5L9kOYkk2k7wryc1JNl+NYgAAAAAAALBbr+URhXclObkcn0xy947xT/W2LyS5qqquTXJ7klPdfa67X0pyKskdr+HfBwAAAAAA4BDabeDqJP+pqp6oquPL2DXd/dxy/HySa5bj65I8s+O7zy5jFxoHAAAAAACAXVvb5by/091nqupvJjlVVX+882J3d1X1fixoCWjHk+Qtb3nLfvwkAAAAAAAAB8iudnB195nl88Ukn8n2O7ReWB49mOXzxWX6mSQ37Pj69cvYhcZ/8N+6v7s3untjfX19b3cDAAAAAADAgXfRwFVVb6yqv/HqcZLbkvxRkkeSHFumHUvy8HL8SJL31bZbkry8PMrw0SS3VdWRqjqy/M6j+3o3AAAAAAAAHHi7eUThNUk+U1Wvzv/t7v79qno8yUNVdW+Sp5O8d5n/2STvSXI6yStJ3p8k3X2uqj6S5PFl3oe7+9y+3QkAAAAAAACHwkUDV3d/M8nbzzP+rSS3nme8k3zgAr/1ySSf3PsyAQAAAAAAYNuu3sEFAAAAAAAAVwqBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBR1la9AHgt6kStegmXXW/2qpcAAAAAAAArZQcXAAAAAAAAowhcAAAAAAAAjCJwAQAAAAAAMIrABQAAAAAAwCgCFwAAAAAAAKMIXAAAAAAAAIwicAEAAAAAADCKwAUAAAAAAMAoAhcAAAAAAACjCFwAAAAAAACMInABAAAAAAAwisAFAAAAAADAKAIXAAAAAAAAowhcAAAAAAAAjCJwAQAAAAAAMIrABQAAAAAAwCgCFwAAAAAAAKMIXAAAAAAAAIwicAEAAAAAADCKwAUAAAAAAMAoAhcAAAAAAACjCFwAAAAAAACMInABAAAAAAAwisAFAAAAAADAKAIXAAAAAAAAowhcAAAAAAAAjCJwAQAAAAAAMIrABQAAAAAAwCgCFwAAAAAAAKMIXAAAAAAAAIwicAEAAAAAADCKwAUAAAAAAMAoAhcAAAAAAACjCFwAAAAAAACMInABAAAAAAAwisAFAAAAAADAKAIXAAAAAAAAowhcAAAAAAAAjCJwAQAAAAAAMIrABQAAAAAAwCgCFwAAAAAAAKMIXAAAAAAAAIwicAEAAAAAADCKwAUAAAAAAMAoAhcAAAAAAACjCFwAAAAAAACMInABAAAAAAAwisAFAAAAAADAKAIXAAAAAAAAowhcAAAAAAAAjCJwAQAAAAAAMIrABQAAAAAAwCgCFwAAAAAAAKMIXAAAAAAAAIwicAEAAAAAADCKwAUAAAAAAMAoAhcAAAAAAACjCFwAAAAAAACMInABAAAAAAAwyq4DV1W9rqq+XFW/t5y/taq+WFWnq+p3q+r1y/gblvPTy/Ubd/zGh5bxb1TV7ft9MwAAAAAAABx8e9nB9YtJvr7j/GNJPt7dP5HkpST3LuP3JnlpGf/4Mi9V9bYk9yT56SR3JPn1qnrda1s+AAAAAAAAh82uAldVXZ/kHyT5jeW8krw7yaeXKSeT3L0c37WcZ7l+6zL/riQPdvd3uvtPk5xOcvN+3AQAAAAAAACHx253cP2bJP8yyf9dzn88ybe7+7vL+bNJrluOr0vyTJIs119e5v/F+Hm+AwAAAAAAALty0cBVVf8wyYvd/cRlWE+q6nhVbVXV1tmzZy/HPwkAAAAAAMAgu9nB9bNJfr6q/izJg9l+NOGvJbmqqtaWOdcnObMcn0lyQ5Is19+U5Fs7x8/znb/Q3fd390Z3b6yvr+/5hgAAAAAAADjYLhq4uvtD3X19d9+Y5J4kn+vuf5Lk80l+YZl2LMnDy/Ejy3mW65/r7l7G76mqN1TVW5McTfKlfbsTAAAAAAAADoW1i0+5oH+V5MGq+uUkX07ywDL+QJLfrKrTSc5lO4qlu79aVQ8l+VqS7yb5QHd/7zX8+wAAAAAAABxCtb256sq0sbHRW1tbq17GGHWiVr0EuCR688r97xQAAAAAAJdGVT3R3Rvnu7abd3ABAAAAAADAFUPgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYJS1VS8A4GLqRK16CZddb/aqlwAAAAAAcMWygwsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGGVt1QsA4IfViVr1Ei673uxVLwEAAAAAGMIOLgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBR1la9AABIkjpRq17CZdebveolAAAAAMBIdnABAAAAAAAwisAFAAAAAADAKBcNXFX1V6vqS1X1X6vqq1V1Yhl/a1V9sapOV9XvVtXrl/E3LOenl+s37vitDy3j36iq2y/VTQEAAAAAAHBw7WYH13eSvLu7357kHUnuqKpbknwsyce7+yeSvJTk3mX+vUleWsY/vsxLVb0tyT1JfjrJHUl+vapet583AwAAAAAAwMF30cDV2/7Hcvpjy18neXeSTy/jJ5PcvRzftZxnuX5rVdUy/mB3f6e7/zTJ6SQ378tdAAAAAAAAcGjs6h1cVfW6qvpKkheTnEryJ0m+3d3fXaY8m+S65fi6JM8kyXL95SQ/vnP8PN8BAAAAAACAXdlV4Oru73X3O5Jcn+1dVz91qRZUVceraquqts6ePXup/hkAAAAAAACG2lXgelV3fzvJ55P8TJKrqmptuXR9kjPL8ZkkNyTJcv1NSb61c/w839n5b9zf3RvdvbG+vr6X5QEAAAAAAHAIrF1sQlWtJ/k/3f3tqvprSX4uyceyHbp+IcmDSY4leXj5yiPL+X9Zrn+uu7uqHkny21X1q0n+VpKjSb60z/cDAGPUiVr1Ei673uxVLwEAAACAA+CigSvJtUlOVtXrsr3j66Hu/r2q+lqSB6vql5N8OckDy/wHkvxmVZ1Oci7JPUnS3V+tqoeSfC3Jd5N8oLu/t7+3AwAAAAAAwEF30cDV3U8meed5xr+Z7fdx/eD4/0ryjy7wW7+S5Ff2vkwAAAAAAADYtqd3cAEAAAAAAMCq7eYRhQAA++KwvXfMO8cAAAAALg07uAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFHWVr0AAICDqk7Uqpdw2fVmr3oJAAAAwCFgBxcAAAAAAACjCFwAAAAAAACMInABAAAAAAAwisAFAAAAAADAKGurXgAAAAdHnahVL+Gy681e9RIAAADg0LGDCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGCUtVUvAAAAJqsTteolXHa92ateAgAAAIecHVwAAAAAAACMInABAAAAAAAwisAFAAAAAADAKAIXAAAAAAAAowhcAAAAAAAAjCJwAQAAAAAAMIrABQAAAAAAwChrq14AAAAwS52oVS/hsuvNXvUSAAAA2MEOLgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEZZW/UCAAAArnR1ola9BC6D3uxVLwEAANglO7gAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEa5aOCqqhuq6vNV9bWq+mpV/eIyfnVVnaqqp5bPI8t4VdUnqup0VT1ZVTft+K1jy/ynqurYpbstAAAAAAAADqrd7OD6bpJ/0d1vS3JLkg9U1duSfDDJY919NMljy3mS3Jnk6PJ3PMl9yXYQS7KZ5F1Jbk6y+WoUAwAAAAAAgN26aODq7ue6+w+W4/+e5OtJrktyV5KTy7STSe5eju9K8qne9oUkV1XVtUluT3Kqu89190tJTiW5Y1/vBgAAAAAAgANvT+/gqqobk7wzyReTXNPdzy2Xnk9yzXJ8XZJndnzt2WXsQuMAAAAAAACwa7sOXFX115P8hyS/1N1/vvNad3eS3o8FVdXxqtqqqq2zZ8/ux08CAAAAAABwgOwqcFXVj2U7bv1Wd//HZfiF5dGDWT5fXMbPJLlhx9evX8YuNP59uvv+7t7o7o319fW93AsAAAAAAACHwEUDV1VVkgeSfL27f3XHpUeSHFuOjyV5eMf4+2rbLUleXh5l+GiS26rqSFUdSXLbMgYAAAAAAAC7traLOT+b5J8m+cOq+soy9q+TfDTJQ1V1b5Knk7x3ufbZJO9JcjrJK0nenyTdfa6qPpLk8WXeh7v73L7cBQAAAAAAAIdGbb8+68q0sbHRW1tbq17GGHWiVr0EAAAABunNK/f/CQAAQFU90d0b57u2q3dwAQAAAAAAwJVC4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGCUtVUvAAAAAFiNOlGrXsJl1Zu96iUAALBP7OACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARvEOLgAAAOBQOGzvHEu8dwwAOLjs4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFEELgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBRBC4AAAAAAABGEbgAAAAAAAAYReACAAAAAABgFIELAAAAAACAUQQuAAAAAAAARhG4AAAAAAAAGEXgAgAAAAAAYBSBCwAAAAAAgFHWVr0AAAAAAC6NOlGrXgKXQW/2qpcAAJedwAUAAAAAgx3GkCnqAeARhQAAAAAAAIwicAEAAAAAADCKwAUAAAAAAMAoAhcAAAAAAACjCFwAAAAAAACMInABAAAAAAAwisAFAAAAAADAKGurXgAAAAAAwF7UiVr1Ei673uxVLwHgiiJwAQAAAABc4UQ9gO/nEYUAAAAAAACMYgcXAAAAAABXnMO2a82ONdgbgQsAAAAAAFbssAW9RNTjtfGIQgAAAAAAAEYRuAAAAAAAABhF4AIAAAAAAGAUgQsAAAAAAIBR1la9AAAAAAAA4PCpE7XqJVx2vdmrXsKBYQcXAAAAAAAAowhcAAAAAAAAjCJwAQAAAAAAMIrABQAAAAAAwCgCFwAAAAAAAKMIXAAAAAAAAIwicAEAAAAAADDKRQNXVX2yql6sqj/aMXZ1VZ2qqqeWzyPLeFXVJ6rqdFU9WVU37fjOsWX+U1V17NLcDgAAAAAAAAfdbnZw/bskd/zA2AeTPNbdR5M8tpwnyZ1Jji5/x5Pcl2wHsSSbSd6V5OYkm69GMQAAAAAAANiLiwau7v7PSc79wPBdSU4uxyeT3L1j/FO97QtJrqqqa5PcnuRUd5/r7peSnMoPRzMAAAAAAAC4qB/1HVzXdPdzy/HzSa5Zjq9L8syOec8uYxcaBwAAAAAAgD35UQPXX+juTtL7sJYkSVUdr6qtqto6e/bsfv0sAAAAAAAAB8SPGrheWB49mOXzxWX8TJIbdsy7fhm70PgP6e77u3ujuzfW19d/xOUBAAAAAABwUP2ogeuRJMeW42NJHt4x/r7adkuSl5dHGT6a5LaqOlJVR5LctowBAAAAAADAnqxdbEJV/U6Sv5fkzVX1bJLNJB9N8lBV3Zvk6STvXaZ/Nsl7kpxO8kqS9ydJd5+rqo8keXyZ9+HuPreP9wEAAAAAAMAh8f/au/twu6r6wOPfHwlVhvcXcXiTKEUBeUkhoFTAKOjglFYrCDL4ktEZilqRCu1oqUOQqvgy1dYXBFKKtYwiIIhIeRkwJgQoryEJYBAkDGWsio+gqai8/OaPtU7uzs25uTfJvefcfc/38zz3ufvss885a529z9prr99aa48a4MrM40d46vAu2ybwvhHe5wLggnVKnSRJkiRJkiRJkjTM+k5RKEmSJEmSJEmSJPWFAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1iskWgakAABjLSURBVAEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktYoBLkmSJEmSJEmSJLWKAS5JkiRJkiRJkiS1igEuSZIkSZIkSZIktUrPA1wRcWRELI+IByPiQ73+fEmSJEmSJEmSJLVbTwNcETEN+CLwBmAv4PiI2KuXaZAkSZIkSZIkSVK79XoE10HAg5n5w8z8LfB14I09ToMkSZIkSZIkSZJarNcBrp2ARxuP/7WukyRJkiRJkiRJksZker8TMFxEnAicWB+ujIjl/UyPxmw74PF+J6LHzPPUN2j5BfM8KMzzYDDPg2HQ8jxo+QXzPCjM82Awz4Nh0PI8aPkF8zwozPNg6EueY270+iPbbteRnuh1gOsxYJfG453rulUy8zzgvF4mShsuIu7IzFn9Tkcvmeepb9DyC+Z5UJjnwWCeB8Og5XnQ8gvmeVCY58FgngfDoOV50PIL5nlQmOfBMIh5nmp6PUXh7cDuEfHiiPgd4K3AlT1OgyRJkiRJkiRJklqspyO4MvOZiPhT4FpgGnBBZt7byzRIkiRJkiRJkiSp3Xp+D67MvBq4utefqwk3iNNKmuepb9DyC+Z5UJjnwWCeB8Og5XnQ8gvmeVCY58FgngfDoOV50PIL5nlQmOfBMIh5nlIiM/udBkmSJEmSJEmSJGnMen0PLkmSJEmSJEmSJGmDGODSOouIrSLivf1Ox0Rr5jMiZkfEVf1OUz9FxMp+p0HjKyJOjoj7I+KifqdlsvK4l7QuIuKFEfG3EbEkIu6KiHkRsUt9bkZELBu2/dyIOK0uvzIi/iUiFteyee6wba+IiFuHrTspIt4xwdlSH0TEzf1Ow3hq1Dkei4gv1HWjHr8RMaezfZfn/nIi0torU20fbwjrpJPL2s5l9fm11o+HtxfU899T9fx2T0TcHBEvq88N/HW21E/d6qeCiLi6lmXDy7OBKbPaXs/aULUOumO/06GxMcCl9bEVMOUDXAxOPjW43gu8LjNPGG3DiOj5PRu1ptEaHNbj/WZFxN/V5dkR8fvr8R4rImK79U2DNFVExG7ANcAiYFZm7g98Dbi8PjearwAnZuZMYG/gG4333go4ANgyIl7SWZ+ZX87Mf+ySlgkps8dSBkXEmyIiI2KPiUhD/YxVZdcEvPekCCRm5jqXx5Pce4HXAad3Vox0/K6DVje8TMF9vCHGXCddV2MI1pweEffW5xdHxCvW83NWq0dFxIURccx45GGUz12jAS4itouIpyPipGHrr67nk7W934aey6D7dfRDmTkzM/ejnO9a/fsdLxExrd9pkLSmzPzPmfkEg90uOOjl9BzAAFdLGODS+jgb2K1eAHy6/i2LiKURcVy/EzeOVuUT+DSwWURcGhHfj4iLIiIAIuKAiPheRNwZEddGxA59TbXGRW3gurNe8J7Y7/SMt4j4MvAS4J8j4tSa3yURcWtE7Fu3mRsRX42IRcBX+5rgDdBtX0bEyoj4WO1FemtEvLCuf3FE3FLLs7/ub8pXN04NDqvJzDsy8+T6cDYwJRrbIuKD9by0LCJOqT0T74+I8+txcF1EbNLvdG6IiPhoRJzSePyxiPhAt3Py8J6GEfGFiJjTh2SPi7o/v18bDx+o5+QjImJRRPwgIg6q/19Qt98oIh7sPJ5A5wDvzMxvZOZvATLzBuBtwP8aw+u3B35UX/dsZt7XeO7NwLeBrwNv7ayM1UeAzY+Iz0XEHcAHIuLhKLaKiGcj4rC63YKI2L1+T7dExN2xem/6BRExs/EZN0XEfhFxPPAw8EbgOeDVdC+Djgduqv/HXURMH1Z2jed7T4pAYn3vlfX/DnWfLK6/7UMn6jMnSrPOAWzdWN88fg+MoQDDp2P13uQ7RsQ19Xf9qbr92cAmdftWjvpp7OPZ9fe7xnXGVNTlHN2sk/7ZOH/WWutOEXEwcBSwf2buCxwBPLqeHzeb/tSj5rBmA9xbgFsZVg43GmxXqeeJZrvQOp3LIuLPI+L2+vs9s65erb2gS5q3AH7e5b1WlQn18bKImFGX3xYRt9X3PDf6EBgar7pXlM5hn4yIuyj7alKr+/jkuvzZiLixLr+2llfnRMQdUerYZzZed3ZE3FePjc/0K/0bIsZW59w0Ii6ox+fdEfHGfqd7PUyLYddJEfHf62/7noi4LCL+Q0RsGRGPdMqMmvdHI2LjWqZeE+W6e2FMYEen8TCG47rTibNbeda1bbDNYlibyVSoZw0XI7QJRMTMKG1CSyLi8ojYOkoHlVnARfU7aHXbwUDITP/8W6c/YAawrC4fDVwPTANeCPxfYId+p3EC8jkbeBLYmRIYvgU4BNgYuBl4Qd3uOOCCfqd9gr6Plf1OQ4/zu039vwmwDNi232magDyuALYDPg+cUde9Flhcl+cCdwKb9Dut470vgQT+sK7/FPBXdflK4B11+X2T6bgHrgP27bJ+D+CKZplV158GzK3L84FPArcBDwCH1vWzgavqa/8NeAxYDBwKvAC4DLi9/r2qvmbbmpZ7gXnAI8B2/f5+Gvk+AFgKbApsVtP5e8AzwMy6zTeAt/U7rRuYzxnAXXV5I+Chkc7Jnf3ceO0XgDn9zsMG5v0ZYJ+a9zuBC4CgBF+uAM4ATqnbvx64bILT9FLgorp8FHAXcGnnc4FvUi6Slg173VzgtLr8PykNfpcDfwI8v7Hd9fV3+VJg6Qivnw98qfHcNcDLa3pup4yceR7wcH1+C2B6XT6ikdZ3Ap9r5OuOuvwThsrHzRqv3QO4orH+sfq65Y20zAa+B3wL+CGlseAESpm0FNitbjdSuTOX0tFiEaVxetUxXT/zH+r7LAGOruvPAe6glAFnNtKyAjiz7qOlwB6N594FfKkeP3+5lu/5c/W9T53AY2pl/X8qcHpdngZs3u/f4HrmZwWlzjEH+EKX73UZcHBdPpuhOvicesxsCTyfcs7ZpfkdtfWvsY9n0+U6o9/pm6A8j3SOXsEE1CUYve70ZuDbI7z2cODumt4LgOfV9avSSinX59O9HnUh8HeUa8UfAsfU13wR+KO6fDn12rGWPx+ry2+jlI+LgXPrb39afc9lNU1/BhwDrASW1203qa9fABwEPAjs3MhT53c4o77mH+s+2LU+P5Zz2XaNY/f1wHmU8+9GlDrlYaxZJ50BPFXT+BClM8eLGsd/pzyfSy0T6uNl9bV7Ujp5bFzXf4l6Purx8TuDcah71f3wF/3+Pa5Dvl8JXFKXF9Zjc2PKufJPGLrWmkb5PexLuV5YDkR9bqt+52MD9vlodc6PU68rKKN9HgA27Xfa1yOPq10n0Wj/AP4aeH9d/hbwmrp8HDCvLt8A7F6XXwHc2O+8jZLv0Y7rFQyVl83ybDZT8JxN9zaTVtezuuRxpGN9CfDquu6jDF0Hzad0jul72v0b/c8RXNpQhwBfy9LT+MeUxosD+5ymiXJbZv5rZj5HqZzPAF5GmUbo+igjvf6KcqJT+50cEfdQej/uAuze5/RMpEOoI7Qy80Zg24jYoj53ZWY+1beUjY9u+/K3lItwKBcqM+ryqyiNpzCJRq1FxEuBn2bmkog4KsoUO5dGxGWZ+X3KaIrRpgmcnpkHAadQKu6rZOYK4MvAZ7NMH7MQ+Nv6+EDKxfu8uvkZwE2Z+XJKw8yLximb4+UQ4PLM/PfMXElpjDmU0qi/uG7T3OetVPfZzyLi9ygNTHczWOfkhzNzaT0n3wvckJlJafSbQWl86Ewp9y5KAGQi7QfcWnuUn0HpLHAqZd8A/IDSoNtNAmTmRymNpdcB/4USoCLKCNPdKb+7B4CnI2LvEd7r4sbyQkpD42HAJyjHx4GUwBGUgMElUUbKfJYSDAO4BDgqIjamfHcX1jLoMeB9EXEu5WL+680yqPZyfSNwTU3nzyLigGHf0UmUhsq3Ay+tZdI84P11m5HKHYC9gCMyc/jIsI8AT2bmPllGYNxY15+embMoDW2vjjo6uXo8y0iOcyidATqOp5wDvsbaR6D9TmbOysyxjMzbULcD/zXKPdn2ycxf9uAzeyrKyLnNM/OWuup/D9vkhsx8MjN/DdwH7NrTBPZGt+uMqWikc/S4G2Pd6TZglzoy40sR8er62udTgknHZeY+wHTgPSN91gj1KCiBjkMowaKz67qFDOV5J0rZRl23ICL2pDQavyrLlLXPUjoEzAR2ysy9a5r+ITMvpQTbT6if+1SU6Rd3yMzbKI13I82wsjulU8TLM/ORum4s57IXN97j9QzVQe6iBA5HumbqTFG4G6Uuet4I23VzOCU4enu95j6cMuqvp8a57nXx6JtMGncCB9Trw99Q6gCzKMfsQuDYOhrtbkpdYi9KAODXwN9HxJuBX/Uj4eNktDrn64EP1WNzPqUzxmS7PhpNt+ukvetIrKWUMqhTT7yYoXLlrcDFEbEZZQTrJfV7OJdS/k1mox3XazMVz9mD0v41/FjfjRKA/15d9xXKtZNaxnuqSGP3m8bys5TfTwD3ZubB/UmSJkJEzKb0Zj84M38VEfMpFdVB9O/9TsCGWMu+fLpemMDQ77kjmXy6NThsSeldBaXBYbSpEb5Z/481uHMEsFdjxoUt6sXLYZQez2TmdyJijSlmJqnhZfhUmGZgHmV0w3+kBHReN8J2z7D6tNRToTxr7s/nGo+fowRzH42IH0fEaym92Mf9vi5dPEsJND+UZQqoJyKiM83g9pSeoFsPe802lGn/AMjMh4BzIuJ84KcRsS1wbH3dw/X3uAUl+HI6a2qW2QsoDbI7UkaH/Tml12nnov0s4LuZ+cdRpoCaX9Pwq4i4nhKsOpbSqHgE5RhbQOkckJQe9N+q79Vp9DyeEqSCMp3i8ZQyB+D2zPwRQEQ8RAnkQWkgek1dHqncgZE7XBxBY9rGzOyUScdGmZZ2OqWRZS9KD01YvTx8c01TM5CYUe5fs3dmdrvxes8aJjNzQZTpJf+AEmz8m9yw+1a1Ubc6+FQzCHnstbHUnXaglHGHUsqhiyPiQ5SG+odrsB5Kg9f7KKM318UVtQH0vlrGQCmDT4mIvSgB262jTHF/MHAyZRRtJ5gDpb7yE8oIppdExOeB7zBUhg53HEP3cPw6pezuFox/JDNv7bJ+tHPZTxrbBvCJzDy3+Qb1nLI2V9K948lI9ZUAvpKZHx7lfXthvOperbnGysynI+JhSr5vppxLXwP8LmVk3mnAgZn584i4kDIC/ZmIOIgSjDwG+FPKb7CN1lrnpPxmjs7M5b1O2Djqdp10IfCmzLwnyvSas+vzVwIfj4htKGXVjZROXE/UoHwrjHJc3z/Ky6fUOXvA2r+G77u13pdS7eEILq2PXwKb1+WFwHERMS3KvS0Oo/SEmwqa+RzJcuAFUeZvJ8rcwy8f5TWa/LYEfl5P7ntQhq9PZQupjb+1cvN4Zv6irykaP+u6Lxcx1FDaiwbxdbFag0PtbTu8wWFtF9KdytxYK+EbAa+svW1nZuZOtbf1ZLcQeFOUeeI3Bf6Y0XvhtdXlwJGUnsLXMvI5+RFK0OB5daTE4f1KcI/NA/6JMv3IsxP8Wcso07E8Tpmnf8uIeBGwZ0TsA2xff7M/qkE3asPAkZT7VRERfxBDkZ3dKb/VJyhBoiMzc0ZmzqA0JryV0d1G6U37XB35spgy5cqC+vyWlFFZUC7um+ZRpta6vREw2poyBdei+t5bsGYZ9FpgXkSsoATUjm3kabQGIlh7uTPmxsCIeDGlwe3wOqrrO6xeJnYrD5uBxBWUjgAjjeLqWcNkROwK/Dgzz6fsl/179dm9UhvRfxkRr6irxnJ8QxnNuPEEJUsTo9fn6FHrTnXkzfzMPIPSCH/0KO/ZDFyM1gDYLPcCIDMfozSoHUkpjxdSyp+VdYRmJ5jTKQdflplza1m8H6UzwkmsPsK16XhgTi3HrgT2jYhuPfG7lWNjPZd1XAu8q9MRISJ2iojtGf06+hDK9H7DraCWcRGxP0OjxW4AjqnvTURsU8vGfhjUutdCynm1c8yeRAkEb0E5lp6sQdw3ANRjYsvMvJoyneZ+/Uh0j1wLvL9T36kj/KaCzSn11o1pXBfXetntlA5NV9Uy9BeU+tNbYNW9/dqwz7se142OsDC2dsG2G6nNZBDqWU8CP4+he9y+nTISFwZj308ZBri0zjLzZ8CiKFPaHEzp6XAPpefGX2Tmv/UzfeNlWD673RyXLDfePQb4ZB3Ou5j+3FxY4+saYHpE3E+ZTqRb78apZC5leP4SSn7f2d/kjKt13ZcfoEzBtZQybcxkMWqDA/D/gO0jYtuIeB5lOpx1MbwCdx1D04YREZ0eeQso06cREW9gzREpfZWZd1F6HN4G/AulAagto8zWST0HfRf4Rg3gXE6Xc3JmPkrpzb2s/r+7T0nutSsZuj/ThMrM+ynT0byMcp+C7wJ/U9NwGmWqPyjTJn6kTt9yI+XeUJ1GvrcDy+tzX6U0JuxCmY5tVdmVmQ9TGpI6wYCR0vQb4NHGaxdSfuNL6+NPAZ+IiLsZFvTOzDuBXzD03S2jNJreAPxRY12zDPpPwFczc9cajNuFMjptXaYfG6ncWZvrKSMrOq/ZmhEa3EaxvoHEiTYbuKfup+MYGiE31bwbOL8e/5tSGhxGcx6wJKbIzc8HQbdzdGZO1DlpLHWn5w8L/sykBCaWAzMi4nfr+maD1wpK+QCrB8PWpSHsVso0fZ1G1dMYCvR1DeZEmQZ2o8y8jDItfifYvepzo0zLuFntHNApyz7B2qdcXWUs57KImE4N3GXmdZQpRW+pdedLKdONrrqOjojOdfRuEbG4XjN/HPhvXZJwGbBNRNxLCTY+UD/nvprn6+r1yvX0afqzAa57LaR857fUqRh/DSzMzHsoefs+5VhYVLffHLiq7q+bgA/2Psk9cxbl3k1L6rF7Vp/TM14+QimnF1H2b9PFlHsXNUe0nwC8u/7G76XMBDDZdT2umxuMUJ5NNSO1mQxKPeudwKdreTWTch8uKPWVL9dz11SY/WVK69zwUZIkTWIR8V1KI+7ulKl2fkiphD9LuZj+cUScTAnSPVafX5GZc+s0A6dl5h21geSOzJxRR+ydlplH1UaRSymjKd5PmZrhi5T75UwHFmTmSVGmTPsaJQB4M2Xe+QMy8/GefBFaJSI2otzz4i2Z+YN+p2eyiYhZlPuhTMj9Xbp83p7ARcD/AP5PXb0/sGNmfrsXaRgvEbEjZZTAHnV6rVHLIMpUWJ/MzGsa73MypQy5mFrW1PXzGSqTZjNUDm1H93JnLmV0w2fq65uv2ay+5oCaljMz85tRpkn6fUqQ70nKFIcX1lENszLz8XqMfIYygm0RsHOz126Ue4q8hxIgW5mZn2mmfUO/Zw2JiM06o/WiTBG3Q2Z+oM/JUsuNodzaGfg8ZUTVM8CDwIm1fDicUj5Mp4xWeE9m/qb28v57SieA+ZTyZHaXetS7KaMbLq1pWZmZnZFO7wbOyswda+/4J4C3Z+Y36/PHAR+mdEh+uubhKUqng04n5Q9n5j9HxNGUgNFTlGDLJpn5ocZ3sC9wcWbu2Sn/KJ0/rsrMNe7nONq5rI7KOD/LPRQHjnUvSZImHwNckiS1wFRqPNeGi3LvjquAyzPz1H6nZ7KpDeTvAU7IzJt6+Lk7U3qZvwKYRhmlcNawKZ0mtYh4B/Ax4IOZeUljvWWQJkyjQX86ZQTNnMz8aX9Tpbaz3Fo/I53LKMH+k4FT6uitgWLdS5KkyckAlyRJLTEVGs8ltZdlkKS2sdySJEma2gxwSZIkSZIkSZIkqVU2Gn0TSZIkSZIkSZIkafIwwCVJkiRJkiRJkqRWMcAlSZIkSZIkSZKkVjHAJUmSJEmSJEmSpFYxwCVJkiRJkiRJkqRW+f87GVixqUAibgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2160x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Length of all text:\")\n",
    "print(len(allText))\n",
    "print(\"Number of unique words:\")\n",
    "print(len(wordCounts))\n",
    "### Begin Part B\n",
    "mostCommon = dict(wordCounts.most_common(25))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,10))\n",
    "ax.bar(mostCommon.keys(), mostCommon.values(), 1, color='g')\n",
    "### End Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about the most common words? Do you think they useful in classifying a review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some of the least common words below. Define the variable least common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Part B\n",
    "leastCommon = dict(wordCounts.most_common()[:-10-1:-1])\n",
    "### End Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thru...': 1, 'credentials.': 1, 'signing': 1, 'volume?': 1, '#disappointing': 1, '#Noaccountability': 1, 'scenes.': 1, 'front\"': 1, '@Gregm528': 1, 'am!': 1}\n"
     ]
    }
   ],
   "source": [
    "print(leastCommon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about the least common words? Do you think they useful in classifying a review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Identifying Unique Most Common Words of Each Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to find the most common words in each classification set that is not included in the other. Basically, we find the most common words in five star reviews that are not in the most common set of words for one star reviews and vice versa. Fill out the below code and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in one star reviews: \n",
      "{'no': 3909, 'went': 2080, 'order': 2016, 'told': 3140, 'did': 2248, 'minutes': 1854, \"don't\": 2405, 'over': 2152, 'she': 3961, 'customer': 1774, \"didn't\": 2615, 'said': 3075, 'never': 2886, 'going': 1762, 'asked': 2314, 'ordered': 1768, 'her': 2675, 'could': 2290, 'who': 1849, 'then': 2142, 'after': 2387, 'came': 2090}\n",
      "\n",
      "Most common words in five star reviews: \n",
      "{'can': 1671, 'Great': 1211, 'best': 1774, 'also': 1553, 'love': 1544, '-': 1372, \"I'm\": 1184, 'always': 1721, 'than': 1038, 'definitely': 1342, 'great': 3288, 'really': 1749, 'recommend': 1257, 'has': 1425, 'friendly': 1167, 'staff': 1198, 'nice': 1136, \"it's\": 1247, 'made': 1148, \"I've\": 1590, 'come': 1071, 'some': 1460}\n"
     ]
    }
   ],
   "source": [
    "allTextFives = ' '.join(dfFives[\"text\"])\n",
    "wordsFives = allTextFives.split() \n",
    "\n",
    "### Begin Part C\n",
    "# Find the 100 most common words that are found in the five star reviews\n",
    "wordCountsFives = Counter()\n",
    "for word in wordsFives:\n",
    "    wordCountsFives[word] += 1\n",
    "    \n",
    "mostCommonFives = dict(wordCountsFives.most_common(100))\n",
    "### End Part C\n",
    "\n",
    "\n",
    "\n",
    "allTextOnes = ' '.join(dfOnes[\"text\"])\n",
    "wordsOnes = allTextOnes.split() \n",
    "\n",
    "### Begin Part C\n",
    "# Find the 100 most common words that are found in the one star reviews\n",
    "wordCountsOnes = Counter()\n",
    "for word in wordsOnes:\n",
    "    wordCountsOnes[word] += 1\n",
    "mostCommonOnes = dict(wordCountsOnes.most_common(100))\n",
    "### End Part C\n",
    "\n",
    "### Begin Part C\n",
    "# Subtract sets in order to find the most common unique words for each set\n",
    "fivesUnique = { k : mostCommonFives[k] for k in set(mostCommonFives) - set(mostCommonOnes) }\n",
    "onesUnique = { k : mostCommonOnes[k] for k in set(mostCommonOnes) - set(mostCommonFives) }\n",
    "### End Part C\n",
    "\n",
    "print(\"Most common words in one star reviews: \")\n",
    "print(onesUnique)\n",
    "print()\n",
    "print(\"Most common words in five star reviews: \")\n",
    "print(fivesUnique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about these words above? Are they more respresentitive of each classification? What words do you thing are good indicators of each review? What words are not so good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect of the bag of words model, we first build a naive baseline model that tries to simply classify the model based on the length of the review. Complete the code below and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def baseline_featurize(review):\n",
    "    ### Begin Part D\n",
    "    # Featurize the data based on the length of the review\n",
    "    return np.asarray([len(review)])\n",
    "    ### End Part D\n",
    "\n",
    "def trainModel(X_featurized, y_true):\n",
    "    ### Begin Part D\n",
    "    # Return a model that uses logistic regression\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_featurized, y_true)\n",
    "    return model\n",
    "    ### End Part D\n",
    "\n",
    "def accuracyData(model, X_featurized, y_true):\n",
    "    ### Begin Part D\n",
    "    # Predict the data given the model and corresponding data. Return the accuracy \n",
    "    # as the percentage of values that were correctly classified. Also print a confusion\n",
    "    # matrix to help visualize the error. Hint: Look at sklearn.metrics.confusion\n",
    "    y_predict = model.predict(X_featurized)\n",
    "    total_num = len(y_true)\n",
    "    total_correct = np.sum([1 if y_predict[i] == y_true[i] else 0 for i in range(len(y_predict))])\n",
    "    total_incorrect = total_num - total_correct\n",
    "    accuracy = total_correct / total_num\n",
    "    print(sklearn.metrics.confusion_matrix(y_true, y_predict, labels=[-1, 1]))\n",
    "    print(accuracy)\n",
    "    ### End Part D\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Train Featurization\n",
      "Beginning Training\n",
      "Beginning Test Featurization\n",
      "Accuracy:\n",
      "[[1800   56]\n",
      " [ 386   66]]\n",
      "0.8084922010398613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8084922010398613"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "featurized_data = np.array(list(map(baseline_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "model = trainModel(featurized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturized_data = np.array(list(map(baseline_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(model, testFeaturized_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did you get as your accuracy? Does that surprise you? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the bag of words model below. Please complete the following code segments and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a wordsOrdered list that contains all words in the train data that shows up more\n",
    "# than one time\n",
    "\n",
    "modifiedCounter = Counter(el for el in wordCounts.elements() if wordCounts[el] > 1)\n",
    "wordsOrdered = [key for key, _ in modifiedCounter.most_common()]\n",
    "\n",
    "def bag_of_words_featurize(review):\n",
    "    ### Begin Part E\n",
    "    # Code the featurization for the bag of words model. Return the corresponding vector\n",
    "    reviewWords = review.split() \n",
    "    vec = np.zeros(len(modifiedCounter))\n",
    "    for word in reviewWords:\n",
    "        if word in wordsOrdered:\n",
    "            vec[wordsOrdered.index(word)] += 1\n",
    "    return vec\n",
    "    ### End Part E        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may\n",
    "around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Train Featurization\n",
      "Beginning Training\n",
      "Beginning Test Featurization\n",
      "Accuracy:\n",
      "[[1791   65]\n",
      " [ 130  322]]\n",
      "0.9155112651646448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9155112651646448"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBagFeaturized_data = np.array(list(map(bag_of_words_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "currBagModel = trainModel(currBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBag_data = np.array(list(map(bag_of_words_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBagModel, testFeaturizedBag_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy? Does that surprise you? Why did it perform as it did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intermed = dict(enumerate(wordsOrdered))\n",
    "wordPosition = {y:x for x,y in intermed.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: Examining Bag of Words Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a function that gets the weight of a word below in the weight vector generated from the bag of words model. Answer the question below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightOfWords(word):\n",
    "    if word not in wordPosition.keys():\n",
    "        print(\"Word does not exist in model, no weight is assigned to it\")\n",
    "        return\n",
    "    return currBagModel.coef_[0][wordPosition[word]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6294962084379272"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try different words here\n",
    "weightOfWords('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List three words that have positive weights. List three that have negative weights. Explain why that makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any set of words that works is sufficient. Ex: 'him', 'her', 'bad' are negatively weighted ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G: Binary Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times when we only want to identify whether a word is in a review or not and disregard the number of times it has shown up in the review. In this case, we find binary bag of words more useful that our regualar bag of words model. Hypothesize which model should run better given the examination of the dataset. Complete the code below and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_binary_featurize(review):\n",
    "    ### Begin Part G\n",
    "    reviewWords = review.split() \n",
    "    vec = np.zeros(len(modifiedCounter))\n",
    "    for word in reviewWords:\n",
    "        if word in wordsOrdered:\n",
    "            vec[wordsOrdered.index(word)] = 1\n",
    "    return vec\n",
    "    ### End Part G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may\n",
    "around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Train Featurization\n",
      "Beginning Training\n",
      "Beginning Test Featurization\n",
      "Accuracy:\n",
      "[[1791   65]\n",
      " [ 128  324]]\n",
      "0.9163778162911612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9163778162911612"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBinBagFeaturized_data = np.array(list(map(bag_of_words_binary_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "currBinBagModel = trainModel(currBinBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBinBag_data = np.array(list(map(bag_of_words_binary_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBinBagModel, testFeaturizedBinBag_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy percentage? Was it what you expected? How did it compare to the regular Bag of Words model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part H: Bag of Words Negative Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times where we also want to identify negative words as negative features instead of regular features. For example if we get a review: \"The food is not good\", the word \"good\" is used in a negative connotation and should be treated as such. Thus we make new features for the negative of each of our chosen words. Complete the code below and answer the following questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_neg_featurize(review):\n",
    "    ### Begin Part H\n",
    "    reviewWords = review.split() \n",
    "    vec = np.zeros(len(modifiedCounter)*2)\n",
    "    isNegative = False\n",
    "    for word in reviewWords:\n",
    "        if word in wordsOrdered:\n",
    "            if isNegative:\n",
    "                vec[wordsOrdered.index(word)+len(modifiedCounter)] += 1\n",
    "            else:\n",
    "                vec[wordsOrdered.index(word)] += 1\n",
    "            isNegative = False\n",
    "        if \"n't\" in word or word == \"not\":\n",
    "            isNegative = True\n",
    "    return vec\n",
    "    ### End Part H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may\n",
    "around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Train Featurization\n",
      "Beginning Training\n",
      "Beginning Test Featurization\n",
      "Accuracy:\n",
      "[[1791   65]\n",
      " [ 130  322]]\n",
      "0.9155112651646448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9155112651646448"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "neg_data = np.array(list(map(bag_of_words_neg_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "negModel = trainModel(neg_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNeg_data = np.array(list(map(bag_of_words_neg_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negModel, testFeaturizedNeg_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did this model perform? Is it as expected? Why did it perform this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bleh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Negative Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the code below and answer the questions below for combining the two features we worked on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_neg_binary_featurize(review):\n",
    "    ### Begin Part I\n",
    "    reviewWords = review.split() \n",
    "    vec = np.zeros(len(modifiedCounter)*2)\n",
    "    isNegative = False\n",
    "    for word in reviewWords:\n",
    "        if word in wordsOrdered:\n",
    "            if isNegative:\n",
    "                vec[wordsOrdered.index(word)+len(modifiedCounter)] = 1\n",
    "            else:\n",
    "                vec[wordsOrdered.index(word)] = 1\n",
    "            isNegative = False\n",
    "        if \"n't\" in word or word == \"not\":\n",
    "            isNegative = True\n",
    "    return vec\n",
    "    ### End Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Train Featurization\n",
      "Beginning Training\n",
      "Beginning Test Featurization\n",
      "Accuracy:\n",
      "[[1788   68]\n",
      " [ 130  322]]\n",
      "0.91421143847487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91421143847487"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "negbin_data = np.array(list(map(bag_of_words_neg_binary_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "negBinModel = trainModel(negbin_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNegBin_data = np.array(list(map(bag_of_words_neg_binary_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negBinModel, testFeaturizedNegBin_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Was the result as expected? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part J (OPTIONAL): Enhanced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get extra credit, Try to create some sort of featurization below that will reach an accuraccy of .97 or higher. Ideas to keep in mind are the Bigram model that was discussed in the notes that takes consecutive words into account as well as methods to increase the number of features we use. Good luck!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-90-40da8d9b339a>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-90-40da8d9b339a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    ### End Part J\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words_extra_credit_featurize(review):\n",
    "    ### Begin Part J\n",
    "    # User solution!\n",
    "    ### End Part J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Train Featurization\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bag_of_words_extra_credit_featurize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-35ec558bc95a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Beginning Train Featurization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mExtraBagFeaturized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_of_words_extra_credit_featurize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Beginning Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mExtraBagModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtraBagFeaturized_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Beginning Test Featurization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bag_of_words_extra_credit_featurize' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "ExtraBagFeaturized_data = np.array(list(map(bag_of_words_extra_credit_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "ExtraBagModel = trainModel(ExtraBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBinBag_extra = np.array(list(map(bag_of_words_extra_credit_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(ExtraBagModel, testFeaturizedBinBag_extra, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA CREDIT:.....HELLo......(BIGRAM REMEMBER) 0.97 REQUIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN EVERY WITH 1-2 and 4-5 STARS\n",
    "# RUN EVERYTHING WITH NEW DATASET\n",
    "# BIGRAMS, NUMPY ARRAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "max_records = 140000\n",
    "data = pd.read_json('yelp_academic_dataset_review.json', lines=True, chunksize = max_records)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for chunk in data:\n",
    "    df = pd.concat([df, chunk])\n",
    "    break\n",
    "    \n",
    "print(type(df))\n",
    "\n",
    "\n",
    "df.to_csv(r'yelp_academic_dataset_review.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONLY RUN BELOW CODE IF YOU ARE ON THE YELP DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'stars'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'stars'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-e198bde09a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get one star reviews and label them with -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfOnes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdfOnes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfOnes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdfOnes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stars'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfOnes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stars'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'stars'"
     ]
    }
   ],
   "source": [
    "### ONLY RUN BELOW CODE IF YOU ARE ON THE YELP DATASET \n",
    "\n",
    "# Get one star reviews and label them with -1\n",
    "dfOnes = df[df['stars'] == 1]\n",
    "dfOnes = dfOnes.head(10000)\n",
    "dfOnes['stars'] = dfOnes['stars'].apply(lambda x: -1)\n",
    "\n",
    "\n",
    "dfTwos = df[df['stars'] == 2]\n",
    "dfTwos = dfTwos.head(10000)\n",
    "dfTwos['stars'] = dfTwos['stars'].apply(lambda x: -1)\n",
    "\n",
    "# Get five star reviews and label them with 1\n",
    "print(\"Shape of the ones input: \")\n",
    "print(dfOnes.shape)\n",
    "\n",
    "dfFives = df[df['stars'] == 5]\n",
    "dfFives = dfFives.head(10000)\n",
    "dfFives['stars'] = dfFives['stars'].apply(lambda x: 1)\n",
    "\n",
    "dfFours = df[df['stars'] == 4]\n",
    "dfFours = dfFours.head(10000)\n",
    "dfFours['stars'] = dfFours['stars'].apply(lambda x: 1)\n",
    "\n",
    "print(\"Shape of the fives input: \")\n",
    "print(dfFives.shape)\n",
    "dfCombined = pd.concat([dfOnes, dfTwos, dfFours, dfFives], axis=0)\n",
    "dfCombined=dfCombined.rename(columns = {'stars':'class'})\n",
    "dfCombined = dfCombined.sample(frac=1)\n",
    "\n",
    "dfTrainset = dfCombined.head(int(len(dfCombined.index) * .8))\n",
    "dfTestset = dfCombined.tail(int(len(dfCombined.index) * .2))\n",
    "\n",
    "trainX = np.asarray(dfTrainset['text'])\n",
    "trainY = np.asarray(dfTrainset['class'])\n",
    "\n",
    "testX = np.asarray(dfTestset['class'])\n",
    "testY = np.asarray(dfTestset['class'])\n",
    "\n",
    "print('Data Frame of reviews:')\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I've been here once and never again. $12 for a ticket? Really? This place doesn't really live up to the price. Also, choosing a seat for a movie that isn't a midnight release is stupid especially if you get in late like I did (bus was late). It makes finding your seat awkward. If I wanted fancy movie experiences and high prices, I'd choose to go to a theatre that should be that way--not one in a shopping mall.\n",
      "\n",
      "Class: -1\n"
     ]
    }
   ],
   "source": [
    "sample = dfTwos.sample() \n",
    "print(\"Text: \" + sample['text'].values[0]  + \"\\n\")\n",
    "print(\"Class: \" + str(sample['class'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I had been wanting to try Joe's Philly Steak & More for a long time, but my husband is vegetarian, so I'd been waiting for a good opportunity to go. Finally, my husband was away for the weekend and Joe's had a Groupon, so I decided that this was my chance. I got the Super Steak sandwich with steak, grilled onion, peppers, and mushrooms and Provolone cheese. I got the smaller size sandwich so that I wouldn't feel too guilty about getting a side dish too... I got the Joe's Potato with steak, grilled onions and cheez whiz. \n",
      "\n",
      "The sandwich and the baked potato were both excellent! The salty meatiness of the steak combined with the savory grilled onions and peppers, added with the grilled mushrooms and creaminess of the Provolone made for a wonderful taste combination! The baked potato was also a great choice. I was not too full, but very satisfied. It was warm and comforting and I would recommend Joe's Philly Steak & More to anyone who loves meat! Plus, they are open Monday thru Saturday until 9 pm, so you can get a late dinner if you want to.\n",
      "\n",
      "Claass: 1\n"
     ]
    }
   ],
   "source": [
    "sample = dfFours.sample() \n",
    "print(\"Text: \" + sample['text'].values[0]  + \"\\n\")\n",
    "print(\"Claass: \" + str(sample['class'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = ' '.join(dfCombined[\"text\"])\n",
    "words = allText.split() \n",
    "\n",
    "wordCounts = Counter()\n",
    "for word in words:\n",
    "    wordCounts[word] += 1\n",
    "    \n",
    "modifiedCounter = Counter(el for el in wordCounts.elements() if wordCounts[el] > 1)\n",
    "wordsOrdered = [key for key, _ in modifiedCounter.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Train Featurization\n",
      "Beginning Training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1362342e7d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcurrBagFeaturized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_of_words_featurize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Beginning Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcurrBagModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrBagFeaturized_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stars\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Beginning Test Featurization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtestFeaturizedBag_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTestset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_of_words_featurize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainModel' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBagFeaturized_data = np.array(list(map(bag_of_words_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "currBagModel = trainModel(currBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBag_data = np.array(list(map(bag_of_words_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBagModel, testFeaturizedBag_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Train Featurization\n",
      "Beginning Training\n",
      "Beginning Test Featurization\n",
      "Accuracy:\n",
      "[[3722  286]\n",
      " [ 327 3665]]\n",
      "0.923375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.923375"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBinBagFeaturized_data = np.array(list(map(bag_of_words_binary_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "currBinBagModel = trainModel(currBinBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBinBag_data = np.array(list(map(bag_of_words_binary_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBinBagModel, testFeaturizedBinBag_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Train Featurization\n",
      "Beginning Training\n",
      "Beginning Test Featurization\n",
      "Accuracy:\n",
      "[[3703  305]\n",
      " [ 301 3691]]\n",
      "0.92425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92425"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "neg_data = np.array(list(map(bag_of_words_neg_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "negModel = trainModel(neg_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNeg_data = np.array(list(map(bag_of_words_neg_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negModel, testFeaturizedNeg_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n",
      "Beginning Test Featurization\n",
      "Accuracy:\n",
      "[[1933   94]\n",
      " [ 100 1873]]\n",
      "0.9515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9515"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "negbin_data = np.array(list(map(bag_of_words_neg_binary_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "negBinModel = trainModel(negbin_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNegBin_data = np.array(list(map(bag_of_words_neg_binary_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negBinModel, testFeaturizedNegBin_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
