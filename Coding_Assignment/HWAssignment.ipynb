{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Yelp Sentiment Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the data set\n",
    "Here we explore the yelp dataset review. We use a dataset with Yelp reviews and a 1-5 star rating associated with it to learn how to use the bag of words model to conduct sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp_academic_dataset_review.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply grab all the one star and five star data from the dataset here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the ones input: \n",
      "(10000, 9)\n",
      "Shape of the fives input: \n",
      "(10000, 9)\n",
      "Data Frame of reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4428</th>\n",
       "      <td>IhNASEZ3XnBHmuuVnWdIwA</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-27 06:50:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2-ZMoSug-pL7unD5u2EwdA</td>\n",
       "      <td>1</td>\n",
       "      <td>Love this tea bar! You HAVE to try their iced ...</td>\n",
       "      <td>0</td>\n",
       "      <td>xTxzruMecRRb2hMQ6QzuMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27667</th>\n",
       "      <td>1jPLv-vyjPSvtl2ntSfdgA</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-05-13 14:45:04</td>\n",
       "      <td>0</td>\n",
       "      <td>_H69rwEWQMY8jxhGHiDtVw</td>\n",
       "      <td>-1</td>\n",
       "      <td>Very unprofessional!!!!  Charged be $650.00 fo...</td>\n",
       "      <td>2</td>\n",
       "      <td>rm72k7hmPx3C2x7NF45KlQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18012</th>\n",
       "      <td>gE_ez1graX93gASNsttgUg</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-27 18:53:52</td>\n",
       "      <td>0</td>\n",
       "      <td>pY2iahDsorXonyILcaHVmQ</td>\n",
       "      <td>1</td>\n",
       "      <td>Fantastic service!  I was in town for a weddin...</td>\n",
       "      <td>1</td>\n",
       "      <td>irJKUrPxs5Z5fTvya-JVyA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186</th>\n",
       "      <td>GwcfhhBoAUT-kJQpDjsHxQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-26 01:58:23</td>\n",
       "      <td>0</td>\n",
       "      <td>9rlrMOfKUznLPhyBHFvbLw</td>\n",
       "      <td>-1</td>\n",
       "      <td>Workers are not friendly, rude and lazy.  They...</td>\n",
       "      <td>0</td>\n",
       "      <td>5wKlVEkZMYmPy6e9p9kPpA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42765</th>\n",
       "      <td>tM_MMrMFwf5rJv9rbu4NQQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-04-02 01:49:51</td>\n",
       "      <td>0</td>\n",
       "      <td>XDlOnf3SRz4UElr3C4tMbw</td>\n",
       "      <td>-1</td>\n",
       "      <td>This is actually a nice location with lots of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>kR-sICS5zHap_0sic6GtTQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54437</th>\n",
       "      <td>vYKZHWBuBT1PUevggLAE9w</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-03 01:21:54</td>\n",
       "      <td>0</td>\n",
       "      <td>qO6mORLcXF6x2a0rOJ7-NA</td>\n",
       "      <td>-1</td>\n",
       "      <td>Bad customer service ever i never see ... Juli...</td>\n",
       "      <td>0</td>\n",
       "      <td>HDE-FpPUd4I6PL53UVsXHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>viji5rHhNlsy9w-JmUhyaw</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-03-06 23:39:47</td>\n",
       "      <td>0</td>\n",
       "      <td>bLuvNJ0uruIC5xZRtUkuZA</td>\n",
       "      <td>1</td>\n",
       "      <td>The service from Nick was amazing. It was my b...</td>\n",
       "      <td>0</td>\n",
       "      <td>_FBFGIcazgiM4j700y_XVg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63846</th>\n",
       "      <td>lpxj6LFir23Ds6swW8a6fg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-28 01:36:48</td>\n",
       "      <td>0</td>\n",
       "      <td>0skWU7rHVrhTDGXnpQ2jMQ</td>\n",
       "      <td>-1</td>\n",
       "      <td>One of the worst meals I've had in a long time...</td>\n",
       "      <td>0</td>\n",
       "      <td>BEv425u9HOYh8QlblM_6jg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>UCpUOtvqR-NBWBNVMzJleA</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-03-18 18:18:41</td>\n",
       "      <td>0</td>\n",
       "      <td>30Jt5_OsmoHlIRgTaSQkpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>We arrived for a couple of drinks after having...</td>\n",
       "      <td>1</td>\n",
       "      <td>MEvfuV0jyJ3H8Db3xw7m8A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18527</th>\n",
       "      <td>CucyObcVxgqte8dH8nLopA</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-24 01:01:26</td>\n",
       "      <td>0</td>\n",
       "      <td>-L5T4g_i53cjUvQn5QqSEQ</td>\n",
       "      <td>-1</td>\n",
       "      <td>What has happened to this location? We have be...</td>\n",
       "      <td>1</td>\n",
       "      <td>P8w4YBgNcYct6BeIut9cEw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  business_id  cool                 date  funny  \\\n",
       "4428   IhNASEZ3XnBHmuuVnWdIwA     1  2017-03-27 06:50:00      0   \n",
       "27667  1jPLv-vyjPSvtl2ntSfdgA     0  2015-05-13 14:45:04      0   \n",
       "18012  gE_ez1graX93gASNsttgUg     0  2016-10-27 18:53:52      0   \n",
       "6186   GwcfhhBoAUT-kJQpDjsHxQ     0  2017-05-26 01:58:23      0   \n",
       "42765  tM_MMrMFwf5rJv9rbu4NQQ     0  2014-04-02 01:49:51      0   \n",
       "...                       ...   ...                  ...    ...   \n",
       "54437  vYKZHWBuBT1PUevggLAE9w     0  2017-01-03 01:21:54      0   \n",
       "3979   viji5rHhNlsy9w-JmUhyaw     0  2016-03-06 23:39:47      0   \n",
       "63846  lpxj6LFir23Ds6swW8a6fg     0  2014-09-28 01:36:48      0   \n",
       "3341   UCpUOtvqR-NBWBNVMzJleA     0  2015-03-18 18:18:41      0   \n",
       "18527  CucyObcVxgqte8dH8nLopA     0  2015-09-24 01:01:26      0   \n",
       "\n",
       "                    review_id  stars  \\\n",
       "4428   2-ZMoSug-pL7unD5u2EwdA      1   \n",
       "27667  _H69rwEWQMY8jxhGHiDtVw     -1   \n",
       "18012  pY2iahDsorXonyILcaHVmQ      1   \n",
       "6186   9rlrMOfKUznLPhyBHFvbLw     -1   \n",
       "42765  XDlOnf3SRz4UElr3C4tMbw     -1   \n",
       "...                       ...    ...   \n",
       "54437  qO6mORLcXF6x2a0rOJ7-NA     -1   \n",
       "3979   bLuvNJ0uruIC5xZRtUkuZA      1   \n",
       "63846  0skWU7rHVrhTDGXnpQ2jMQ     -1   \n",
       "3341   30Jt5_OsmoHlIRgTaSQkpg     -1   \n",
       "18527  -L5T4g_i53cjUvQn5QqSEQ     -1   \n",
       "\n",
       "                                                    text  useful  \\\n",
       "4428   Love this tea bar! You HAVE to try their iced ...       0   \n",
       "27667  Very unprofessional!!!!  Charged be $650.00 fo...       2   \n",
       "18012  Fantastic service!  I was in town for a weddin...       1   \n",
       "6186   Workers are not friendly, rude and lazy.  They...       0   \n",
       "42765  This is actually a nice location with lots of ...       1   \n",
       "...                                                  ...     ...   \n",
       "54437  Bad customer service ever i never see ... Juli...       0   \n",
       "3979   The service from Nick was amazing. It was my b...       0   \n",
       "63846  One of the worst meals I've had in a long time...       0   \n",
       "3341   We arrived for a couple of drinks after having...       1   \n",
       "18527  What has happened to this location? We have be...       1   \n",
       "\n",
       "                      user_id  \n",
       "4428   xTxzruMecRRb2hMQ6QzuMA  \n",
       "27667  rm72k7hmPx3C2x7NF45KlQ  \n",
       "18012  irJKUrPxs5Z5fTvya-JVyA  \n",
       "6186   5wKlVEkZMYmPy6e9p9kPpA  \n",
       "42765  kR-sICS5zHap_0sic6GtTQ  \n",
       "...                       ...  \n",
       "54437  HDE-FpPUd4I6PL53UVsXHg  \n",
       "3979   _FBFGIcazgiM4j700y_XVg  \n",
       "63846  BEv425u9HOYh8QlblM_6jg  \n",
       "3341   MEvfuV0jyJ3H8Db3xw7m8A  \n",
       "18527  P8w4YBgNcYct6BeIut9cEw  \n",
       "\n",
       "[20000 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get one star reviews and label them with -1\n",
    "dfOnes = df[df['stars'] == 1]\n",
    "dfOnes = dfOnes.head(10000)\n",
    "dfOnes['stars'] = dfOnes['stars'].apply(lambda x: -1)\n",
    "\n",
    "# Get five star reviews and label them with 1\n",
    "print(\"Shape of the ones input: \")\n",
    "print(dfOnes.shape)\n",
    "dfFives = df[df['stars'] == 5]\n",
    "dfFives = dfFives.head(10000)\n",
    "dfFives['stars'] = dfFives['stars'].apply(lambda x: 1)\n",
    "\n",
    "print(\"Shape of the fives input: \")\n",
    "print(dfFives.shape)\n",
    "dfCombined = pd.concat([dfOnes, dfFives], axis=0)\n",
    "dfCombined = dfCombined.sample(frac=1)\n",
    "\n",
    "dfTrainset = dfCombined.head(int(len(dfCombined.index) * .8))\n",
    "dfTestset = dfCombined.tail(int(len(dfCombined.index) * .2))\n",
    "\n",
    "trainX = np.asarray(dfTrainset['text'])\n",
    "trainY = np.asarray(dfTrainset['stars'])\n",
    "\n",
    "testX = np.asarray(dfTestset['text'])\n",
    "testY = np.asarray(dfTestset['stars'])\n",
    "\n",
    "print('Data Frame of reviews:')\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Review Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run the below block multiple times to see different reviews and the classification assigned to the review. Please comment below on what interesting aspects of the reviews you find to lead to certain classifications. What distinguishes between a classification of 1 and one of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: We recently stopped in to sit at the bar and have some appetizers and drinks.  The food was awesome (beef Wellington bites and chili paneer). Robbie was a great bartender. Very knowledgeable and great to talk to while sitting at the bar.\n",
      "\n",
      "Classification: 1\n"
     ]
    }
   ],
   "source": [
    "sample = dfCombined.sample() \n",
    "print(\"Text: \" + sample['text'].values[0]  + \"\\n\")\n",
    "print(\"Classification: \" + str(sample['stars'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any meaningful answer that discusses words that are more commonly used in positive reviews vs\n",
    "# negative reviews or length analysis will suffice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Corpus Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking of the combined string of every review in the usable dataset. In the provided space below use a histogram to visualize the frequency of the 25 most common words. Then answer the questions that follow. Hint: The most_common funtion for the counter may come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allText = ' '.join(dfCombined[\"text\"])\n",
    "words = allText.split() \n",
    "\n",
    "wordCounts = Counter()\n",
    "for word in words:\n",
    "    wordCounts[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all text:\n",
      "11676339\n",
      "Number of unique words:\n",
      "101317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 25 artists>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Length of all text:\")\n",
    "print(len(allText))\n",
    "print(\"Number of unique words:\")\n",
    "print(len(wordCounts))\n",
    "### Begin Part B\n",
    "mostCommon = dict(wordCounts.most_common(25))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.bar(mostCommon.keys(), mostCommon.values(), 1, color='g')\n",
    "### End Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about the most common words? Do you think they useful in classifying a review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some of the least common words below. Define the variable least common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Part B\n",
    "leastCommon = dict(wordCounts.most_common()[:-10-1:-1])\n",
    "### End Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'back...what': 1, 'cared....': 1, 'Busy..': 1, 'later...NO': 1, 'server..': 1, 'on..had': 1, 'nearby..': 1, '$200.00.': 1, '(uncommon)': 1, 'ect).': 1}\n"
     ]
    }
   ],
   "source": [
    "print(leastCommon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about the least common words? Do you think they useful in classifying a review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Identifying Unique Most Common Words of Each Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to find the most common words in each classification set that is not included in the other. Basically, we find the most common words in five star reviews that are not in the most common set of words for one star reviews and vice versa. Fill out the below code and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in one star reviews: \n",
      "{'then': 2142, 'told': 3140, 'said': 3075, 'came': 2090, 'order': 2016, 'after': 2387, 'she': 3961, 'over': 2152, 'her': 2675, 'customer': 1774, 'did': 2248, 'ordered': 1768, 'went': 2080, 'could': 2290, \"don't\": 2405, 'going': 1762, 'minutes': 1854, 'no': 3909, 'asked': 2314, 'who': 1849, 'never': 2886, \"didn't\": 2615}\n",
      "\n",
      "Most common words in five star reviews: \n",
      "{'some': 1460, 'always': 1721, 'staff': 1198, '-': 1372, 'made': 1148, 'come': 1071, 'recommend': 1257, 'great': 3288, 'Great': 1211, 'friendly': 1167, \"it's\": 1247, \"I've\": 1590, 'has': 1425, 'can': 1671, 'definitely': 1342, \"I'm\": 1184, 'nice': 1136, 'also': 1553, 'love': 1544, 'than': 1038, 'best': 1774, 'really': 1749}\n"
     ]
    }
   ],
   "source": [
    "allTextFives = ' '.join(dfFives[\"text\"])\n",
    "wordsFives = allTextFives.split() \n",
    "\n",
    "### Begin Part C\n",
    "# Find the 100 most common words that are found in the five star reviews\n",
    "wordCountsFives = Counter()\n",
    "for word in wordsFives:\n",
    "    wordCountsFives[word] += 1\n",
    "    \n",
    "mostCommonFives = dict(wordCountsFives.most_common(100))\n",
    "### End Part C\n",
    "\n",
    "\n",
    "\n",
    "allTextOnes = ' '.join(dfOnes[\"text\"])\n",
    "wordsOnes = allTextOnes.split() \n",
    "\n",
    "### Begin Part C\n",
    "# Find the 100 most common words that are found in the one star reviews\n",
    "wordCountsOnes = Counter()\n",
    "for word in wordsOnes:\n",
    "    wordCountsOnes[word] += 1\n",
    "mostCommonOnes = dict(wordCountsOnes.most_common(100))\n",
    "### End Part C\n",
    "\n",
    "### Begin Part C\n",
    "# Subtract sets in order to find the most common unique words for each set\n",
    "fivesUnique = { k : mostCommonFives[k] for k in set(mostCommonFives) - set(mostCommonOnes) }\n",
    "onesUnique = { k : mostCommonOnes[k] for k in set(mostCommonOnes) - set(mostCommonFives) }\n",
    "### End Part C\n",
    "\n",
    "print(\"Most common words in one star reviews: \")\n",
    "print(onesUnique)\n",
    "print()\n",
    "print(\"Most common words in five star reviews: \")\n",
    "print(fivesUnique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about these words above? Are they more respresentitive of each classification? What words do you thing are good indicators of each review? What words are not so good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect of the bag of words model, we first build a naive baseline model that tries to simply classify the model based on the length of the review. Complete the code below and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def baseline_featurize(review):\n",
    "    ### Begin Part D\n",
    "    # Featurize the data based on the length of the review\n",
    "    return np.asarray([len(review)])\n",
    "    ### End Part D\n",
    "\n",
    "def trainModel(X_featurized, y_true):\n",
    "    ### Begin Part D\n",
    "    # Return a model that uses logistic regression\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_featurized, y_true)\n",
    "    return model\n",
    "    ### End Part D\n",
    "\n",
    "def accuracyData(model, X_featurized, y_true):\n",
    "    ### Begin Part D\n",
    "    # Predict the data given the model and corresponding data. Return the accuracy \n",
    "    # as the percentage of values that were correctly classified. Also print a confusion\n",
    "    # matrix to help visualize the error. Hint: Look at sklearn.metrics.confusion\n",
    "    y_predict = model.predict(X_featurized)\n",
    "    total_num = len(y_true)\n",
    "    total_correct = np.sum([1 if y_predict[i] == y_true[i] else 0 for i in range(len(y_predict))])\n",
    "    total_incorrect = total_num - total_correct\n",
    "    accuracy = total_correct / total_num\n",
    "    print(sklearn.metrics.confusion_matrix(y_true, y_predict, labels=[-1, 1]))\n",
    "    print(accuracy)\n",
    "    ### End Part D\n",
    "    return accuracy\n",
    "    \n",
    "featurized_data = np.stack(np.asarray(dfCombined[\"text\"].apply(baseline_featurize)))\n",
    "y_true = np.asarray(dfCombined[\"stars\"])\n",
    "\n",
    "# print(featurized_data.shape)\n",
    "# print(y_true.shape)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(featurized_data, y_true)\n",
    "    \n",
    "    \n",
    "\n",
    "# currFeaturized_data = np.stack(np.asarray(dfTrainset[\"text\"].apply(baseline_featurize)))\n",
    "# currModel = trainModel(currFeaturized_data, np.asarray(dfTrainset[\"stars\"]))\n",
    "\n",
    "# testFeaturized_data = np.stack(np.asarray(dfTestset[\"text\"].apply(baseline_featurize)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# predictData(currModel, testFeaturized_data, np.asarray(dfTestset[\"stars\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1, ...,  1,  1, -1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracyData(model, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did you get as your accuracy? Does that surprise you? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the bag of words model below. Please complete the following code segments and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a wordsOrdered list that contains all words in the train data that shows up more\n",
    "# than one time\n",
    "\n",
    "modifiedCounter = Counter(el for el in wordCounts.elements() if wordCounts[el] > 1)\n",
    "wordsOrdered = [key for key, _ in modifiedCounter.most_common()]\n",
    "\n",
    "def bag_of_words_featurize(review):\n",
    "    ### Begin Part E\n",
    "    # Code the featurization for the bag of words model. Return the corresponding vector\n",
    "    reviewWords = review.split() \n",
    "    vec = np.zeros(len(modifiedCounter))\n",
    "    for word in reviewWords:\n",
    "        if word in wordsOrdered:\n",
    "            vec[wordsOrdered.index(word)] += 1\n",
    "    return vec\n",
    "    ### End Part E        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may\n",
    "around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "HELLO2\n",
      "HELLO3\n",
      "0.95875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95875"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBagFeaturized_data = np.stack(np.asarray(dfTrainset[\"text\"].apply(bag_of_words_featurize)))\n",
    "print(\"Beginning Training\")\n",
    "currBagModel = trainModel(currBagFeaturized_data, np.asarray(dfTrainset[\"stars\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBag_data = np.stack(np.asarray(dfTestset[\"text\"].apply(bag_of_words_featurize)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBagModel, testFeaturizedBag_data, np.asarray(dfTestset[\"stars\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy? Does that surprise you? Why did it perform as it did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intermed = dict(enumerate(wordsOrdered))\n",
    "wordPosition = {y:x for x,y in intermed.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: Examining Bag of Words Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a function that gets the weight of a word below in the weight vector generated from the bag of words model. Answer the question below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightOfWords(word):\n",
    "    if word not in wordPosition.keys():\n",
    "        print(\"Word does not exist in model, no weight is assigned to it\")\n",
    "        return\n",
    "    return currBagModel.coef_[0][wordPosition[word]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different words here\n",
    "weightOfWords('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List three words that have positive weights. List three that have negative weights. Explain why that makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any set of words that works is sufficient. Ex: 'him', 'her', 'bad' are negatively weighted ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G: Binary Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times when we only want to identify whether a word is in a review or not and disregard the number of times it has shown up in the review. In this case, we find binary bag of words more useful that our regualar bag of words model. Hypothesize which model should run better given the examination of the dataset. Complete the code below and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_binary_featurize(review):\n",
    "    ### Begin Part G\n",
    "    reviewWords = review.split() \n",
    "    vec = np.zeros(len(modifiedCounter))\n",
    "    for word in reviewWords:\n",
    "        if word in wordsOrdered:\n",
    "            vec[wordsOrdered.index(word)] = 1\n",
    "    return vec\n",
    "    ### End Part G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may\n",
    "around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "HELLO2\n",
      "HELLO3\n",
      "0.95475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95475"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBinBagFeaturized_data = np.stack(np.asarray(dfTrainset[\"text\"].apply(bag_of_words_binary_featurize)))\n",
    "print(\"Beginning Training\")\n",
    "currBinBagModel = trainModel(currBinBagFeaturized_data, np.asarray(dfTrainset[\"stars\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBinBag_data = np.stack(np.asarray(dfTestset[\"text\"].apply(bag_of_words_binary_featurize)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBinBagModel, testFeaturizedBinBag_data, np.asarray(dfTestset[\"stars\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy percentage? Was it what you expected? How did it compare to the regular Bag of Words model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part H: Bag of Words Negetive Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times where we also want to identify negative words as negative features instead of regular features. For example if we get a review: \"The food is not good\", the word \"good\" is used in a negative connotation and should be treated as such. Thus we make new features for the negative of each of our chosen words. Complete the code below and answer the following questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_neg_featurize(review):\n",
    "    ### Begin Part H\n",
    "    reviewWords = review.split() \n",
    "    vec = np.zeros(len(modifiedCounter)*2)\n",
    "    isNegative = False\n",
    "    for word in reviewWords:\n",
    "        if word in wordsOrdered:\n",
    "            if isNegative:\n",
    "                vec[wordsOrdered.index(word)+len(modifiedCounter)] += 1\n",
    "            else:\n",
    "                vec[wordsOrdered.index(word)] += 1\n",
    "            isNegative = False\n",
    "        if \"n't\" in word or word == \"not\":\n",
    "            isNegative = True\n",
    "    return vec\n",
    "    ### End Part H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may\n",
    "around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "HELLO2\n",
      "HELLO3\n",
      "0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "neg_data = np.stack(np.asarray(dfTrainset[\"text\"].apply(bag_of_words_neg_featurize)))\n",
    "print(\"Beginning Training\")\n",
    "negModel = trainModel(neg_data, np.asarray(dfTrainset[\"stars\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNeg_data = np.stack(np.asarray(dfTestset[\"text\"].apply(bag_of_words_neg_featurize)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negModel, testFeaturizedNeg_data, np.asarray(dfTestset[\"stars\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did this model perform? Is it as expected? Why did it perform this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bleh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Negative Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the code below and answer the questions below for combining the two features we worked on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_neg_binary_featurize(review):\n",
    "    ### Begin Part I\n",
    "    reviewWords = review.split() \n",
    "    vec = np.zeros(len(modifiedCounter)*2)\n",
    "    isNegative = False\n",
    "    for word in reviewWords:\n",
    "        if word in wordsOrdered:\n",
    "            if isNegative:\n",
    "                vec[wordsOrdered.index(word)+len(modifiedCounter)] = 1\n",
    "            else:\n",
    "                vec[wordsOrdered.index(word)] = 1\n",
    "            isNegative = False\n",
    "        if \"n't\" in word or word == \"not\":\n",
    "            isNegative = True\n",
    "    return vec\n",
    "    ### End Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "HELLO2\n",
      "HELLO3\n",
      "[[1927   78]\n",
      " [  93 1902]]\n",
      "0.95725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95725"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "negbin_data = np.stack(np.asarray(dfTrainset[\"text\"].apply(bag_of_words_neg_binary_featurize)))\n",
    "print(\"Beginning Training\")\n",
    "negBinModel = trainModel(negbin_data, np.asarray(dfTrainset[\"stars\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNegBin_data = np.stack(np.asarray(dfTestset[\"text\"].apply(bag_of_words_neg_binary_featurize)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negBinModel, testFeaturizedNegBin_data, np.asarray(dfTestset[\"stars\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Was the result as expected? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part J (OPTIONAL): Enhanced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get extra credit, Try to create some sort of featurization below that will reach an accuraccy of .97 or higher. Ideas to keep in mind are the Bigram model that was discussed in the notes that takes consecutive words into account as well as methods to increase the number of features we use. Good luck!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_extra_credit_featurize(review):\n",
    "    ### Begin Part J\n",
    "    # User solution!\n",
    "    ### End Part J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "ExtraBagFeaturized_data = np.stack(np.asarray(dfTrainset[\"text\"].apply(bag_of_words_extra_credit_featurize)))\n",
    "print(\"Beginning Training\")\n",
    "ExtraBagModel = trainModel(ExtraBagFeaturized_data, np.asarray(dfTrainset[\"stars\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBinBag_extra = np.stack(np.asarray(dfTestset[\"text\"].apply(bag_of_words_extra_credit_featurize)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(ExtraBagModel, testFeaturizedBinBag_extra, np.asarray(dfTestset[\"stars\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA CREDIT:.....HELLo......(BIGRAM REMEMBER) 0.97 REQUIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-acba803a3ee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mlines_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# Make sure that the returned objects have the right index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'frame'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 853\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    854\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             decoded = {str(k): v for k, v in compat.iteritems(\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# RUN EVERY WITH 1-2 and 4-5 STARS\n",
    "# RUN EVERYTHING WITH NEW DATASET\n",
    "# BIGRAMS, NUMPY ARRAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "max_records = 140000\n",
    "data = pd.read_json('yelp_academic_dataset_review.json', lines=True, chunksize = max_records)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for chunk in data:\n",
    "    df = pd.concat([df, chunk])\n",
    "    break\n",
    "    \n",
    "print(type(df))\n",
    "\n",
    "\n",
    "df.to_csv(r'yelp_academic_dataset_review.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
