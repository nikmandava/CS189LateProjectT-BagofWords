{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bag of Words for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: In this problem we provide an introduction to a real world application of the Bag of Words Model: sentiment analysis and binary classification. The student will perform binary classification on two datasets: one Yelp Review dataset partitioned by low and highly rated reviews and another dataset of Airplane Tweets classified into positive and negative sentiment. The student starts off with data exploration. Afterwards, they create a simple logistic regression model with the only feature being word count, then implement bag of words features, then explore a number of modifications to the model in order to evaluate the tangible impact of using different variations and better understand the nuances of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the data set\n",
    "Here we explore the dataset of Yelp Reviews and Airplane Tweets. The Yelp dataset has a star rating of 1 to 5, so we extract the most polar reviews with 1 and 5 stars and classify them as -1 and 1 respectively. For Airplane Tweets, we translate the 'negative' and 'positive' sentiment labels into classes of -1 and 1. We initially load the data and do some exploratory analysis in order to learn more about the data we will be classifying and gain some insight into how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment to use Yelp Reviews dataset\n",
    "df = pd.read_csv('yelp_academic_dataset_review.csv') \n",
    "###\n",
    "\n",
    "### Uncomment this to use the Airplane Tweets dataset\n",
    "# df = pd.read_csv('Tweets.csv') \n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply grab all the one star and five star data from the dataset here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Uncomment to use Yelp Reviews dataset\n",
    "# Get one star reviews and label them with -1\n",
    "dfNegative = df[df['stars'] == 1]\n",
    "dfNegative = dfNegative.head(10000)\n",
    "dfNegative['stars'] = dfNegative['stars'].apply(lambda x: -1)\n",
    "\n",
    "# Get five star reviews and label them with 1\n",
    "print(\"Shape of the negative input: \")\n",
    "print(dfNegative.shape)\n",
    "dfPositive = df[df['stars'] == 5]\n",
    "dfPositive = dfPositive.head(10000)\n",
    "dfPositive['stars'] = dfPositive['stars'].apply(lambda x: 1)\n",
    "\n",
    "print(\"Shape of the positive input: \")\n",
    "print(dfPositive.shape)\n",
    "dfCombined = pd.concat([dfNegative, dfPositive], axis=0)\n",
    "dfCombined = dfCombined[['stars', 'text']]\n",
    "dfCombined=dfCombined.rename(columns = {'stars':'class'})\n",
    "###\n",
    "\n",
    "\n",
    "### Uncomment this to use the Airplane Tweets dataset\n",
    "# dfCombined = df[['airline_sentiment', 'text']]\n",
    "# dfCombined = dfCombined[dfCombined.airline_sentiment != 'neutral']\n",
    "# dfCombined['airline_sentiment'] = dfCombined['airline_sentiment'].replace(['positive','negative'],[1, -1])\n",
    "# dfCombined = dfCombined.rename(columns = {'airline_sentiment':'class'})\n",
    "# dfPositive = dfCombined[dfCombined['class'] == 1]\n",
    "# dfNegative = dfCombined[dfCombined['class'] == -1]\n",
    "# print(\"Shape of the negative input: \")\n",
    "# print(dfNegative.shape)\n",
    "# print(\"Shape of the positive input: \")\n",
    "# print(dfPositive.shape)\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "# Randomly shuffling the data then dividing it into train and test sets\n",
    "dfCombined = dfCombined.sample(frac=1)\n",
    "print(\"Shape of the dataframe: \")\n",
    "print(dfCombined.shape)\n",
    "\n",
    "dfTrainset = dfCombined.head(int(len(dfCombined.index) * .8))\n",
    "dfTestset = dfCombined.tail(int(len(dfCombined.index) * .2))\n",
    "\n",
    "trainX = np.asarray(dfTrainset['text'])\n",
    "trainY = np.asarray(dfTrainset['class'])\n",
    "\n",
    "testX = np.asarray(dfTestset['text'])\n",
    "testY = np.asarray(dfTestset['class'])\n",
    "\n",
    "print('Data Frame of reviews:')\n",
    "\n",
    "\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run the below block multiple times to see different reviews and their respective class. Please comment below on what interesting aspects of the reviews you find associated with each class. What distinguishes between a classification of 1 and one of -1? Do so for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dfTrainset.sample() \n",
    "print(\"Text: \" + sample['text'].values[0]  + \"\\n\")\n",
    "print(\"Classification: \" + str(sample['class'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "    \n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Corpus Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at all the text in our train dataset (corpus) in order to see what it contains. In the provided space below use a histogram to visualize the frequency of the 25 most common words. Then answer the questions that follow. Hint: The most_common function for Counters may come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allText = ' '.join(dfTrainset[\"text\"])\n",
    "words = allText.split() \n",
    "\n",
    "wordCounts = Counter()\n",
    "for word in words:\n",
    "    wordCounts[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of all text:\")\n",
    "print(len(allText))\n",
    "print(\"Number of unique words:\")\n",
    "print(len(wordCounts))\n",
    "### Begin Part B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about the most common words for both datasets? Do you think they are useful in classifying a review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some of the least common words below. Define the variable least common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Part B\n",
    "\n",
    "### End Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(leastCommon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about the least common words for both datasets? Do you think they are useful in classifying a review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Identifying Unique Most Common Words of Each Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to find the most common words in each class that are not included in the other. Basically, we find the most common words in positive reviews (class = 1) that are not in the most common set of words for negative reviews (class = -1) and vice versa. Fill out the below code and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTextPositive = ' '.join(dfPositive[\"text\"])\n",
    "wordsPositive = allTextPositive.split() \n",
    "\n",
    "### Begin Part C\n",
    "# Find the 100 most common words that are found in the five star reviews\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End Part C\n",
    "\n",
    "\n",
    "\n",
    "allTextNegative = ' '.join(dfNegative[\"text\"])\n",
    "wordsNegative = allTextNegative.split() \n",
    "\n",
    "### Begin Part C\n",
    "# Find the 100 most common words that are found in the one star reviews\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End Part C\n",
    "\n",
    "### Begin Part C\n",
    "# Subtract sets in order to find the most common unique words for each set\n",
    "\n",
    "\n",
    "\n",
    "### End Part C\n",
    "\n",
    "print(\"Most common words in negative reviews: \")\n",
    "print(negativeUnique)\n",
    "print()\n",
    "print(\"Most common words in positive reviews: \")\n",
    "print(positiveUnique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about these words above? Are they more respresentative of each classification? What words do you think are good indicators of each review class? What words are not so good? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "    \n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constructing and Evaluating Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect of the bag of words model, we first build a naive baseline model that tries to simply classification of the model purely based on the length of the review. Complete the code below and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def baseline_featurize(review):\n",
    "    ### Begin Part D\n",
    "    # Featurize the data based on the length of the review. Hint: There should only be one feature.\n",
    "\n",
    "    ### End Part D\n",
    "\n",
    "def trainModel(X_featurized, y_true):\n",
    "    ### Begin Part D\n",
    "    # Return a logistic regression model\n",
    "\n",
    "    \n",
    "\n",
    "    ### End Part D\n",
    "\n",
    "def accuracyData(model, X_featurized, y_true):\n",
    "    ### Begin Part D\n",
    "    # Predict the data given the model and corresponding data. Print and return the accuracy \n",
    "    # as the percentage of values that were correctly classified. Also print a confusion\n",
    "    # matrix to help visualize the error. Hint: Look at sklearn.metrics.confusion\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### End Part D\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Part D\n",
    "# Featurize the training data and then train a model on it. \n",
    "# Afterwards, featurize the test data and evaluate the model on it.\n",
    "# Use the functions you made above to do so\n",
    "print(\"Beginning Train Featurization\")\n",
    "\n",
    "print(\"Beginning Training\")\n",
    "\n",
    "print(\"Beginning Test Featurization\")\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "\n",
    "### End Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did you get as your accuracy? Does that surprise you? Why or why not? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the bag of words featurization below based on the provided lecture. Please complete the following code segments and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a wordsOrdered list that contains all words in the train data that show up more\n",
    "# than one time. Each word count should be in its respective place in the feature vector.\n",
    "\n",
    "modifiedCounter = Counter(el for el in wordCounts.elements() if wordCounts[el] > 1)\n",
    "wordsOrdered = [key for key, _ in modifiedCounter.most_common()]\n",
    "\n",
    "def bag_of_words_featurize(review):\n",
    "    ### Begin Part E\n",
    "    # Code the featurization for the bag of words model. Return the corresponding vector\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### End Part E        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may\n",
    "around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBagFeaturized_data = np.array(list(map(bag_of_words_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "currBagModel = trainModel(currBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBag_data = np.array(list(map(bag_of_words_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBagModel, testFeaturizedBag_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy? Does that surprise you? Why did it perform as it did? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intermed = dict(enumerate(wordsOrdered))\n",
    "wordPosition = {y:x for x,y in intermed.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: Examining Bag of Words Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a function that gets the weight of a word feature below in the weight vector generated from the logistic regression model with bag of words featurization. Answer the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightOfWords(word):\n",
    "    if word not in wordPosition.keys():\n",
    "        print(\"Word does not exist in model, no weight is assigned to it\")\n",
    "        return\n",
    "    return currBagModel.coef_[0][wordPosition[word]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different words here\n",
    "weightOfWords('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List three words that have positive weights. List three that have negative weights. Explain why that makes sense. Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G: Binary Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times when we only want to identify whether a word is in a review or not and disregard the number of times it has shown up in the review. In this case, we find binary bag of words more useful that our regualar bag of words model. Hypothesize which model should run better given the examination of the dataset. Complete the code below and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_binary_featurize(review):\n",
    "    ### Begin Part G\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### End Part G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may\n",
    "around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBinBagFeaturized_data = np.array(list(map(bag_of_words_binary_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "currBinBagModel = trainModel(currBinBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBinBag_data = np.array(list(map(bag_of_words_binary_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBinBagModel, testFeaturizedBinBag_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy percentage? Was it what you expected? How did it compare to the regular Bag of Words model? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part H: Bag of Words Negative Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times where we also want to identify negative words as separate features instead of regular features. For example if we get a review: \"The food is not good\", the word \"good\" is used in a negative connotation and should be treated as such. Thus we make new features for the negative of each of our chosen words. Complete the code below and answer the following questions. Hint: Try doubling the size of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_neg_featurize(review):\n",
    "    ### Begin Part H\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### End Part H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may\n",
    "around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "neg_data = np.array(list(map(bag_of_words_neg_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "negModel = trainModel(neg_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNeg_data = np.array(list(map(bag_of_words_neg_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negModel, testFeaturizedNeg_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did this model perform? Is it as expected? Why did it perform this way? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Negative Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the code below and answer the questions below for combining the two features we worked on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_neg_binary_featurize(review):\n",
    "    ### Begin Part I\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### End Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below script and see how well the bag of words model performs. Warning: this block may around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "negbin_data = np.array(list(map(bag_of_words_neg_binary_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "negBinModel = trainModel(negbin_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNegBin_data = np.array(list(map(bag_of_words_neg_binary_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negBinModel, testFeaturizedNegBin_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Was the result as expected? Why or why not? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part J (OPTIONAL): Enhanced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get extra credit, Try to create some sort of featurization below that will reach an accuracy of .97 or higher for either model. Ideas to keep in mind are the Bigram model that was discussed in the notes that takes consecutive words into account as well as methods to increase the number of features we use. Good luck!!\n",
    "HINT: You can combine additional features like length with existing bag of words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_extra_credit_featurize(review):\n",
    "    ### Begin Part J\n",
    "    # User solution!\n",
    "    ### End Part J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "ExtraBagFeaturized_data = np.array(list(map(bag_of_words_extra_credit_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "ExtraBagModel = trainModel(ExtraBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBinBag_extra = np.array(list(map(bag_of_words_extra_credit_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(ExtraBagModel, testFeaturizedBinBag_extra, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What features did you add? Why did you do so? What was your accuracy percentage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESPONSE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">ONLY RUN BELOW CODE IF YOU ARE ON THE YELP DATASET</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Yelp Model with Less Polar Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be performing a similar analysis on the Yelp Dataset but including both 1 star and 2 star reviews as the negative class and 4 star and 5 star reviews as the positive class. This way there will be less of a clear divide between the two classes and students should see how adapting the bag of words model can prove beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get one star reviews and label them with -1\n",
    "dfOnes = df[df['stars'] == 1]\n",
    "dfOnes = dfOnes.head(10000)\n",
    "dfOnes['stars'] = dfOnes['stars'].apply(lambda x: -1)\n",
    "\n",
    "\n",
    "dfTwos = df[df['stars'] == 2]\n",
    "dfTwos = dfTwos.head(10000)\n",
    "dfTwos['stars'] = dfTwos['stars'].apply(lambda x: -1)\n",
    "\n",
    "# Get five star reviews and label them with 1\n",
    "print(\"Shape of the ones input: \")\n",
    "print(dfOnes.shape)\n",
    "\n",
    "dfFives = df[df['stars'] == 5]\n",
    "dfFives = dfFives.head(10000)\n",
    "dfFives['stars'] = dfFives['stars'].apply(lambda x: 1)\n",
    "\n",
    "dfFours = df[df['stars'] == 4]\n",
    "dfFours = dfFours.head(10000)\n",
    "dfFours['stars'] = dfFours['stars'].apply(lambda x: 1)\n",
    "\n",
    "print(\"Shape of the fives input: \")\n",
    "print(dfFives.shape)\n",
    "dfCombined = pd.concat([dfOnes, dfTwos, dfFours, dfFives], axis=0)\n",
    "dfCombined=dfCombined.rename(columns = {'stars':'class'})\n",
    "dfCombined = dfCombined.sample(frac=1)\n",
    "\n",
    "dfTrainset = dfCombined.head(int(len(dfCombined.index) * .8))\n",
    "dfTestset = dfCombined.tail(int(len(dfCombined.index) * .2))\n",
    "\n",
    "trainX = np.asarray(dfTrainset['text'])\n",
    "trainY = np.asarray(dfTrainset['class'])\n",
    "\n",
    "testX = np.asarray(dfTestset['class'])\n",
    "testY = np.asarray(dfTestset['class'])\n",
    "\n",
    "print('Data Frame of reviews:')\n",
    "dfCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part K: Data Sampling of the 2 and 4 star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dfTwos.sample() \n",
    "print(\"Text: \" + sample['text'].values[0]  + \"\\n\")\n",
    "print(\"Class: \" + str(sample['class'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dfFours.sample() \n",
    "print(\"Text: \" + sample['text'].values[0]  + \"\\n\")\n",
    "print(\"Class: \" + str(sample['class'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = ' '.join(dfCombined[\"text\"])\n",
    "words = allText.split() \n",
    "\n",
    "wordCounts = Counter()\n",
    "for word in words:\n",
    "    wordCounts[word] += 1\n",
    "    \n",
    "modifiedCounter = Counter(el for el in wordCounts.elements() if wordCounts[el] > 1)\n",
    "wordsOrdered = [key for key, _ in modifiedCounter.most_common()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part L: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBaselineFeaturized_data = np.array(list(map(baseline_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "currBaselineModel = trainModel(currBaselineFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBaseline_data = np.array(list(map(baseline_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBaselineModel, testFeaturizedBaseline_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy percentage? Was it what you expected? How did it compare to the regular Bag of Words model? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part M: Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBagFeaturized_data = np.array(list(map(bag_of_words_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "currBagModel = trainModel(currBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBag_data = np.array(list(map(bag_of_words_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBagModel, testFeaturizedBag_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy percentage? Was it what you expected? How did it compare to the regular Bag of Words model? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part N: Binary Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "currBinBagFeaturized_data = np.array(list(map(bag_of_words_binary_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "currBinBagModel = trainModel(currBinBagFeaturized_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedBinBag_data = np.array(list(map(bag_of_words_binary_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(currBinBagModel, testFeaturizedBinBag_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy percentage? Was it what you expected? How did it compare to the regular Bag of Words model? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part O: Negative Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "neg_data = np.array(list(map(bag_of_words_neg_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "negModel = trainModel(neg_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNeg_data = np.array(list(map(bag_of_words_neg_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negModel, testFeaturizedNeg_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy percentage? Was it what you expected? How did it compare to the regular Bag of Words model? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part P: Negative Binary Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Train Featurization\")\n",
    "negbin_data = np.array(list(map(bag_of_words_neg_binary_featurize, trainX)))\n",
    "print(\"Beginning Training\")\n",
    "negBinModel = trainModel(negbin_data, np.asarray(dfTrainset[\"class\"]))\n",
    "print(\"Beginning Test Featurization\")\n",
    "testFeaturizedNegBin_data = np.array(list(map(bag_of_words_neg_binary_featurize, testX)))\n",
    "print(\"Accuracy:\")\n",
    "accuracyData(negBinModel, testFeaturizedNegBin_data, np.asarray(dfTestset[\"class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was your accuracy percentage? Was it what you expected? How did it compare to the regular Bag of Words model? Answer for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp Reviews:\n",
    "\n",
    "Airplane Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
