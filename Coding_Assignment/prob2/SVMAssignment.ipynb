{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning SVM Bag of Words Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk is a great library for NLP that we can use to get items such as a list of common stop words as you will see below\n",
    "- keras does a good job making it easy for us to set up the bag of words model easily by simply feeding in text rather than implementing it ourselves as we did in the first HW question\n",
    "- sklearn is a library we use to simply build ML models we have discussed in this class such as SVMs and Logistic Regression. It will alsoo allow us to see the confusion matrix and get accuracy easily. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question we use IMBD movie reviews from Kaggle and attempt to use SVM (which we learned about earlier in this class) and the bag of words model to do sentiment analysis. We also examine the effect of data cleaning on train/test accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Identifying and Adding Cleaning functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get an example review and attempt to create a function to clean the data. Below Identify what the three given cleaning sections do and explain why they are helpful, and write code for a fourth section that would aid in removing words such as \"I\" or \"A\" which do not have an impact on sentiment analysis. \n",
    "\n",
    "Hint: Consider the minimum length of useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting the doc\n",
    "def get_doc(filename):\n",
    "    f = open(filename, 'r')\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    return txt\n",
    "\n",
    "# Used if we did not clean file\n",
    "def not_clean_file(f):\n",
    "    data = f.split()\n",
    "    return data\n",
    "\n",
    "# function for cleaning the doc\n",
    "def clean_file(f):\n",
    "    # we grab all the data seperated by whitespace\n",
    "    data = f.split()\n",
    "    \n",
    "    # Clean 1  \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    data = [w.translate(table) for w in data]\n",
    "    \n",
    "    #  Clean 2\n",
    "    data = [w for w in data if w.isalpha()]\n",
    "    \n",
    "    # Clean 3\n",
    "    # Recall from our notes what stop words are. Here we use the stopwords library to easily gain access\n",
    "    # to a list of common stop words in english. EX. words include: \n",
    "    #{‘ourselves’, ‘hers’, ‘between’, ‘yourself’, ‘but’, ‘again’, ‘there’, ‘about’, ‘once’, ‘during’, ‘out’}\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data = [w for w in data if not w in stop_words]\n",
    "    \n",
    "    ### Begin Part A\n",
    "    # Add code here\n",
    "    ### End Part A\n",
    "    return data\n",
    "    \n",
    "\n",
    "# get the cleaned text\n",
    "f = 'data/pos/cv000_29590.txt'\n",
    "text = get_doc(f)\n",
    "print(\"Original text: \")\n",
    "print(text[:1000])\n",
    "\n",
    "cleaned_text = clean_file(text)\n",
    "not_cleaned_text = not_clean_file(text)\n",
    "\n",
    "print()\n",
    "print(\"Words from text: \")\n",
    "print(not_cleaned_text[:30])\n",
    "\n",
    "print()\n",
    "print(\"Cleaned words from text: \")\n",
    "print(cleaned_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 1:\n",
    "# \n",
    "# Clean 2:\n",
    "# \n",
    "# Clean 3:\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training without Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Building Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a vocabulary that we can use for later steps. To do this we run the functions from before for all the train data. For this part of the assignment we wil NOT be cleaning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab (not clean)\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    doc = get_doc(filename)\n",
    "    not_cleaned = not_clean_file(doc)\n",
    "    vocab.update(not_cleaned)\n",
    "    \n",
    "\n",
    "def process_docs(directory, vocab):\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        \n",
    "# define vocab as a counter type\n",
    "vocab = Counter()\n",
    "# Adding both positive and negative data\n",
    "process_docs('data/pos', vocab)\n",
    "process_docs('data/neg', vocab)\n",
    "# Printing the most common words from vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the most common values in the vocabulary above. Do you think that they are helpful in our sentiment analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: removing values that appear less than once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to include words that appear only once in our vocabulary as they are most likely unique words that are not common and do not play a major role in sentiment analysis. Write the ccode below to remove all words with length less than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start Part C\n",
    "min_occurance = 5\n",
    "trim_vocab = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "### End Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save this vocab as a file to use for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "# save tokens to a vocabulary file\n",
    "save_file(trim_vocab, 'vocab_unclean.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create helper functions that will allow  us to properly use SVMs as our learning models. Please complete the code segments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function we will use to load a doc and grab all values that are also\n",
    "# in vocab\n",
    "def doc_to_line(filename, vocab):\n",
    "    doc = get_doc(filename)\n",
    "    words = not_clean_file(doc)\n",
    "    # Write code to only include words that are in the vocabulary\n",
    "    ### Begin Part D\n",
    "    # Add code here\n",
    "    ### End Part D\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all data given whether it is train or test\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        # choose train or test data\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "            \n",
    "        path = directory + '/' + filename\n",
    "        line = doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use tokenizer in order to generate our Xtrain and Xtest\n",
    "# This uses the Tokenizer library in order to help create the featurizations for the bag of words model based on the\n",
    "# input words and vocabulary we have.\n",
    "\n",
    "# the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\n",
    "\n",
    "# word_counts: A dictionary of words and their counts.\n",
    "# word_docs: A dictionary of words and how many documents each appeared in.\n",
    "# word_index: A dictionary of words and their uniquely assigned integers.\n",
    "# document_count:An integer count of the total number of documents that were used to fit the Tokenizer.\n",
    "\n",
    "# This function provides a suite of standard bag-of-words model text encoding schemes that can be provided \n",
    "# via a mode argument to the function.\n",
    "\n",
    "# The modes available include:\n",
    "\n",
    "# ‘binary‘: Whether or not each word is present in the document. This is the default.\n",
    "# ‘count‘: The count of each word in the document.\n",
    "# ‘tfidf‘: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
    "# ‘freq‘: The frequency of each word as a ratio of words within each document.\n",
    "\n",
    "# For more information about Tokenizer, you can visit: \n",
    "# https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "\n",
    "# View this link: https://keras.io/api/preprocessing/text/#tokenizer \n",
    "# For documentation information\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    # We create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode train data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode test data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Running the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now begin to run the model. Please complete the code below and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab_unclean.txt'\n",
    "vocab = get_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "positive_lines = process_docs('data/pos', vocab, True)\n",
    "negative_lines = process_docs('data/neg', vocab, True)\n",
    "train_docs = negative_lines + positive_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lines = process_docs('data/pos', vocab, False)\n",
    "negative_lines = process_docs('data/neg', vocab, False)\n",
    "test_docs = negative_lines + positive_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "ytrain = np.array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "ytest = np.array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note here we use the binary option. If you read the explanation for Tokenizer, you see there are three other options.\n",
    "# Feel free to try 'count', 'freq', or 'tfidf' instead, but we do not require you to do so for this problem\n",
    "Xtrain, Xtest = prepare_data(train_docs, test_docs, 'binary')\n",
    "# Write code below to use SVMs to create a model. You may use sklearn\n",
    "### Begin Part E\n",
    "# Add code here\n",
    "### End Part E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train error\n",
    "### Begin Part E\n",
    "# Add code here\n",
    "### End Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test error\n",
    "### Begin Part E\n",
    "# Add code here\n",
    "### End Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Confusion Matrix\n",
    "### Begin Part E\n",
    "# Add code here\n",
    "### End Part E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did your model perform. Is it what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: Vocabulary with Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will recreate our vocabulary but with cleaned data this data. Respond to the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab (clean)\n",
    "def add_doc_to_vocab2(filename, vocab):\n",
    "    doc = get_doc(filename)\n",
    "    cleaned = clean_file(doc)\n",
    "    vocab.update(cleaned)\n",
    "    \n",
    "def process_docs2(directory, vocab):\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab2(path, vocab)\n",
    "        \n",
    "# define vocab as a counter type\n",
    "vocab2 = Counter()\n",
    "# Adding both positive and negative data\n",
    "process_docs2('data/pos', vocab2)\n",
    "process_docs2('data/neg', vocab2)\n",
    "# Printing the most common words from vocab\n",
    "print(vocab2.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the most common words compare to that of Part B when we built the vocabulary without cleaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-add the code from Part C below to remove values that appear less than five times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start Part F\n",
    "min_occurance = 5\n",
    "trim_vocab2 = [k for k,c in vocab2.items() if c >= min_occurance]\n",
    "### End Part F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to a vocabulary file\n",
    "save_file(trim_vocab2, 'vocab_clean.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G: Training with Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-train the model with clean data this time. Add code below and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function we will use to load a doc and grab all values that are also\n",
    "# in vocab\n",
    "def doc_to_line2(filename, vocab):\n",
    "    doc = get_doc(filename)\n",
    "    words = clean_file(doc)\n",
    "    # Write code to only include words that are in the vocabulary\n",
    "    # This is the same as part D\n",
    "    ### Begin Part G\n",
    "    # Add code here\n",
    "    ### End Part G\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Loads all data given whether it is train or test\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        # choose train or test data\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "            \n",
    "        path = directory + '/' + filename\n",
    "        line = doc_to_line2(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab_clean.txt'\n",
    "vocab = get_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "positive_lines = process_docs('data/pos', vocab, True)\n",
    "negative_lines = process_docs('data/neg', vocab, True)\n",
    "train_docs = negative_lines + positive_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lines = process_docs('data/pos', vocab, False)\n",
    "negative_lines = process_docs('data/neg', vocab, False)\n",
    "test_docs = negative_lines + positive_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "ytrain = np.array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "ytest = np.array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest = prepare_data(train_docs, test_docs, 'binary')\n",
    "# Write code below to use SVMs to create a model. You may use sklearn\n",
    "# Same as Part E\n",
    "### Begin Part G\n",
    "# Add code here\n",
    "### End Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train error\n",
    "# Same as Part E\n",
    "### Begin Part G\n",
    "# Add code here\n",
    "### End Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test error\n",
    "# Same as Part E\n",
    "### Begin Part G\n",
    "# Add code here\n",
    "### End Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Confusion Matrix\n",
    "# Same as Part E\n",
    "### Begin Part G\n",
    "# Add code here\n",
    "### End Part G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you expect these results? What effect did cleaning the data before training have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
