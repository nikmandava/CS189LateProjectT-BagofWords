{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning SVM Bag of Words Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question we use IMBD movie reviews from Kaggle and attempt to use SVM (which we learned about earlier in this class) and the bag of words model to do sentiment analysis. We also examine the effect of data cleaning on train/test accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Identifying and Adding Cleaning functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get an example review and attempt to create a function to clean the data. Below Identify what the three given cleaning sections do and explain why they are helpful, and write code for a fourth section that would aid in removing words such as \"I\" or \"A\" which do not have an impact on sentiment analysis. \n",
    "\n",
    "Hint: Consider the minimum length of useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      "films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
      "for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \n",
      "to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \n",
      "the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . \n",
      "in other words , don't dismiss this film because of its source . \n",
      "if you can get past the whole comic book thing , you might find another stumbling block in from hell's directors , albert and allen hughes . \n",
      "getting the hughes brothers to direct this seems almost as \n",
      "\n",
      "Words from text: \n",
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', \"they're\", 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(', 'casper', ')']\n",
      "\n",
      "Cleaned words from text: \n",
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world']\n"
     ]
    }
   ],
   "source": [
    "# function for getting the doc\n",
    "def get_doc(filename):\n",
    "    f = open(filename, 'r')\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    return txt\n",
    "\n",
    "# Used if we did not clean file\n",
    "def not_clean_file(f):\n",
    "    data = f.split()\n",
    "    return data\n",
    "\n",
    "# function for cleaning the doc\n",
    "def clean_file(f):\n",
    "    # we grab all the data seperated by whitespace\n",
    "    data = f.split()\n",
    "    \n",
    "    # Clean 1  \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    data = [w.translate(table) for w in data]\n",
    "    \n",
    "    #  Clean 2\n",
    "    data = [w for w in data if w.isalpha()]\n",
    "    \n",
    "    # Clean 3\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data = [w for w in data if not w in stop_words]\n",
    "    \n",
    "    ### Begin Part A\n",
    "    data = [word for word in data if len(word) > 1]\n",
    "    ### End Part A\n",
    "    return data\n",
    "    \n",
    "\n",
    "# get the cleaned text\n",
    "f = 'data/pos/cv000_29590.txt'\n",
    "text = get_doc(f)\n",
    "print(\"Original text: \")\n",
    "print(text[:1000])\n",
    "\n",
    "cleaned_text = clean_file(text)\n",
    "not_cleaned_text = not_clean_file(text)\n",
    "\n",
    "print()\n",
    "print(\"Words from text: \")\n",
    "print(not_cleaned_text[:30])\n",
    "\n",
    "print()\n",
    "print(\"Cleaned words from text: \")\n",
    "print(cleaned_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 1:\n",
    "# We first remove punctuation from each token. This is useful since\n",
    "# punctuation would get added at the end of strings making them seem\n",
    "# like unique words while they are not.\n",
    "# Clean 2:\n",
    "# We remove any words that are not alphabetic. We assume that symbols and\n",
    "# numbers do not contribute as much to sentiment analysis and remove them.\n",
    "# Clean 3:\n",
    "# We filter out stop words. If we pring out the stop words, we see they \n",
    "# are a list of words that do not add sentiment value and thus can be\n",
    "# removed without impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training without Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Building Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a vocabulary that we can use for later steps. To do this we run the functions from before for all the train data. For this part of the assignment we wil NOT be cleaning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 69706), ('the', 68273), ('.', 59103), ('a', 34138), ('and', 31642), ('of', 30419), ('to', 28503), ('is', 22501), ('in', 19410), ('\"', 15798), ('that', 13536), ('it', 11068), (')', 10577), ('(', 10467), ('as', 10175), ('with', 9651), ('for', 8880), ('his', 8602), ('this', 8563), ('film', 7974), ('but', 7727), ('he', 6816), ('i', 6710), ('on', 6479), ('are', 6232), ('by', 5608), ('be', 5468), ('an', 5099), ('one', 4939), ('not', 4913), ('who', 4859), ('movie', 4815), ('at', 4464), ('was', 4420), ('from', 4417), ('have', 4400), ('has', 4266), ('you', 4010), ('her', 3963), ('they', 3849), ('all', 3819), ('?', 3397), (\"it's\", 3348), ('so', 3238), ('like', 3193), ('about', 3141), ('out', 3071), ('more', 2991), ('when', 2957), ('which', 2849)]\n"
     ]
    }
   ],
   "source": [
    "# load doc and add to vocab (not clean)\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    doc = get_doc(filename)\n",
    "    not_cleaned = not_clean_file(doc)\n",
    "    vocab.update(not_cleaned)\n",
    "    \n",
    "\n",
    "def process_docs(directory, vocab):\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        \n",
    "# define vocab as a counter type\n",
    "vocab = Counter()\n",
    "# Adding both positive and negative data\n",
    "process_docs('data/pos', vocab)\n",
    "process_docs('data/neg', vocab)\n",
    "# Printing the most common words from vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the most common values in the vocabulary above. Do you think that they are helpful in our sentiment analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We notice that the most common words seem to be punctuation, one letter\n",
    "# words and other words such as the and a that do not add value to our \n",
    "# sentiment analysis. This is why we do cleaning on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: removing values that appear less than once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to include words that appear only once in our vocabulary as they are most likely unique words that are not common and do not play a major role in sentiment analysis. Write the ccode below to remove all words with length less than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start Part C\n",
    "min_occurance = 5\n",
    "trim_vocab = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "### End Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save this vocab as a file to use for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "# save tokens to a vocabulary file\n",
    "save_file(trim_vocab, 'vocab_unclean.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create helper functions that will allow  us to properly use SVMs as our learning models. Please complete the code segments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function we will use to load a doc and grab all values that are also\n",
    "# in vocab\n",
    "def doc_to_line(filename, vocab):\n",
    "    doc = get_doc(filename)\n",
    "    words = not_clean_file(doc)\n",
    "    # Write code to only include words that are in the vocabulary\n",
    "    ### Begin Part D\n",
    "    words = [w for w in words if w in vocab]\n",
    "    ### End Part D\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all data given whether it is train or test\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        # choose train or test data\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "            \n",
    "        path = directory + '/' + filename\n",
    "        line = doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use tokenizer in order to generate our Xtrain and Xtest\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    # We create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode train data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode test data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Running the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now begin to run the model. Please complete the code below and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab_unclean.txt'\n",
    "vocab = get_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "positive_lines = process_docs('data/pos', vocab, True)\n",
    "negative_lines = process_docs('data/neg', vocab, True)\n",
    "train_docs = negative_lines + positive_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lines = process_docs('data/pos', vocab, False)\n",
    "negative_lines = process_docs('data/neg', vocab, False)\n",
    "test_docs = negative_lines + positive_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "ytrain = np.array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "ytest = np.array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Xtest = prepare_data(train_docs, test_docs, 'binary')\n",
    "# Write code below to use SVMs to create a model. You may use sklearn\n",
    "### Begin Part E\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, ytrain)\n",
    "### End Part E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922222222222222"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print train error\n",
    "### Begin Part E\n",
    "clf.score(Xtrain, ytrain)\n",
    "### End Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print test error\n",
    "### Begin Part E\n",
    "clf.score(Xtest, ytest)\n",
    "### End Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91  9]\n",
      " [24 76]]\n"
     ]
    }
   ],
   "source": [
    "# Print Confusion Matrix\n",
    "### Begin Part E\n",
    "ypred = clf.predict(Xtest)\n",
    "print(confusion_matrix(ytest, ypred))\n",
    "### End Part E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did your model perform. Is it what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any answer here with sufficient thought will suffice. EX: The train and\n",
    "# test error were expected given this model because ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: Vocabulary with Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will recreate our vocabulary but with cleaned data this data. Respond to the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "# load doc and add to vocab (clean)\n",
    "def add_doc_to_vocab2(filename, vocab):\n",
    "    doc = get_doc(filename)\n",
    "    cleaned = clean_file(doc)\n",
    "    vocab.update(cleaned)\n",
    "    \n",
    "def process_docs2(directory, vocab):\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab2(path, vocab)\n",
    "        \n",
    "# define vocab as a counter type\n",
    "vocab2 = Counter()\n",
    "# Adding both positive and negative data\n",
    "process_docs2('data/pos', vocab2)\n",
    "process_docs2('data/neg', vocab2)\n",
    "# Printing the most common words from vocab\n",
    "print(vocab2.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the most common words compare to that of Part B when we built the vocabulary without cleaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words are much more relevant to sentiment analysis. We do not see\n",
    "# punctuation anymore or words that dont have meaning to sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-add the code from Part C below to remove values that appear less than five times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start Part F\n",
    "min_occurance = 5\n",
    "trim_vocab2 = [k for k,c in vocab2.items() if c >= min_occurance]\n",
    "### End Part F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to a vocabulary file\n",
    "save_file(trim_vocab2, 'vocab_clean.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G: Training with Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-train the model with clean data this time. Add code below and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function we will use to load a doc and grab all values that are also\n",
    "# in vocab\n",
    "def doc_to_line2(filename, vocab):\n",
    "    doc = get_doc(filename)\n",
    "    words = clean_file(doc)\n",
    "    # Write code to only include words that are in the vocabulary\n",
    "    # This is the same as part D\n",
    "    ### Begin Part G\n",
    "    words = [w for w in words if w in vocab]\n",
    "    ### End Part G\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Loads all data given whether it is train or test\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        # choose train or test data\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "            \n",
    "        path = directory + '/' + filename\n",
    "        line = doc_to_line2(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab_clean.txt'\n",
    "vocab = get_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all training reviews\n",
    "positive_lines = process_docs('data/pos', vocab, True)\n",
    "negative_lines = process_docs('data/neg', vocab, True)\n",
    "train_docs = negative_lines + positive_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lines = process_docs('data/pos', vocab, False)\n",
    "negative_lines = process_docs('data/neg', vocab, False)\n",
    "test_docs = negative_lines + positive_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "ytrain = np.array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "ytest = np.array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Xtest = prepare_data(train_docs, test_docs, 'binary')\n",
    "# Write code below to use SVMs to create a model. You may use sklearn\n",
    "# Same as Part E\n",
    "### Begin Part G\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(Xtrain, ytrain)\n",
    "### End Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9911111111111112"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print train error\n",
    "# Same as Part E\n",
    "### Begin Part G\n",
    "clf.score(Xtrain, ytrain)\n",
    "### End Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print test error\n",
    "# Same as Part E\n",
    "### Begin Part G\n",
    "clf.score(Xtest, ytest)\n",
    "### End Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91  9]\n",
      " [18 82]]\n"
     ]
    }
   ],
   "source": [
    "# Print Confusion Matrix\n",
    "# Same as Part E\n",
    "### Begin Part G\n",
    "ypred = clf.predict(Xtest)\n",
    "print(confusion_matrix(ytest, ypred))\n",
    "### End Part G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you expect these results? What effect did cleaning the data before training have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any thoughtful answer that explores the impact of cleaning on reducing the total list of words and the impact that \n",
    "# it has is sufficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
